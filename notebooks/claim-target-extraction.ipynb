{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src-py/')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus    \n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_debater_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/'\n",
    "model_folder = '../../data-ceph/arguana/arg-generation/claim-target-tagger/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Target tagger on IBM dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-20 12:53:07,539 Reading data from ../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds\n",
      "2022-06-20 12:53:07,540 Train: ../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/train_ds.tsv\n",
      "2022-06-20 12:53:07,540 Dev: None\n",
      "2022-06-20 12:53:07,541 Test: ../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/test_ds.tsv\n"
     ]
    }
   ],
   "source": [
    "columns = {0: 'text', 1: 'pos', 2: 'ct'}\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train_ds.tsv',\n",
    "                              test_file='test_ds.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-20 12:53:14,830 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157/1157 [00:00<00:00, 30142.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-20 12:53:14,872 Corpus contains the labels: pos (#14298), ct (#14298)\n",
      "2022-06-20 12:53:14,873 Created (for label 'ct') Dictionary with 4 tags: <unk>, B-CT, I-CT, O\n",
      "Dictionary with 4 tags: <unk>, B-CT, I-CT, O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-20 12:53:52,636 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:53:52,705 Model: \"SequenceTagger(\n",
      "  (embeddings): TransformerWordEmbeddings(\n",
      "    (model): XLMRobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 1024)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (18): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (19): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (20): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (21): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (22): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (23): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (linear): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-06-20 12:53:52,750 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:53:52,772 Corpus: \"Corpus: 1157 train + 129 dev + 974 test sentences\"\n",
      "2022-06-20 12:53:52,788 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:53:52,815 Parameters:\n",
      "2022-06-20 12:53:52,818  - learning_rate: \"5e-06\"\n",
      "2022-06-20 12:53:52,822  - mini_batch_size: \"4\"\n",
      "2022-06-20 12:53:52,827  - patience: \"3\"\n",
      "2022-06-20 12:53:52,831  - anneal_factor: \"0.5\"\n",
      "2022-06-20 12:53:52,836  - max_epochs: \"10\"\n",
      "2022-06-20 12:53:52,839  - shuffle: \"True\"\n",
      "2022-06-20 12:53:52,852  - train_with_dev: \"False\"\n",
      "2022-06-20 12:53:52,856  - batch_growth_annealing: \"False\"\n",
      "2022-06-20 12:53:52,860 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:53:52,874 Model training base path: \"../../data-ceph/arguana/arg-generation/claim-target-tagger/model\"\n",
      "2022-06-20 12:53:52,887 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:53:52,891 Device: cuda:0\n",
      "2022-06-20 12:53:52,895 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:53:52,899 Embeddings storage mode: none\n",
      "2022-06-20 12:53:52,925 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:53:56,864 epoch 1 - iter 29/290 - loss 1.39938321 - samples/sec: 29.50 - lr: 0.000001\n",
      "2022-06-20 12:54:00,683 epoch 1 - iter 58/290 - loss 1.36496464 - samples/sec: 30.56 - lr: 0.000001\n",
      "2022-06-20 12:54:04,502 epoch 1 - iter 87/290 - loss 1.32545335 - samples/sec: 30.56 - lr: 0.000002\n",
      "2022-06-20 12:54:08,293 epoch 1 - iter 116/290 - loss 1.26901156 - samples/sec: 30.79 - lr: 0.000002\n",
      "2022-06-20 12:54:12,042 epoch 1 - iter 145/290 - loss 1.21278089 - samples/sec: 31.09 - lr: 0.000003\n",
      "2022-06-20 12:54:15,855 epoch 1 - iter 174/290 - loss 1.15068652 - samples/sec: 30.57 - lr: 0.000003\n",
      "2022-06-20 12:54:19,703 epoch 1 - iter 203/290 - loss 1.09818949 - samples/sec: 30.27 - lr: 0.000003\n",
      "2022-06-20 12:54:23,522 epoch 1 - iter 232/290 - loss 1.04912509 - samples/sec: 30.50 - lr: 0.000004\n",
      "2022-06-20 12:54:27,378 epoch 1 - iter 261/290 - loss 0.99246797 - samples/sec: 30.21 - lr: 0.000005\n",
      "2022-06-20 12:54:31,247 epoch 1 - iter 290/290 - loss 0.94991901 - samples/sec: 30.11 - lr: 0.000005\n",
      "2022-06-20 12:54:31,255 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:54:31,259 EPOCH 1 done: loss 0.9499 - lr 0.0000050\n",
      "2022-06-20 12:54:33,356 DEV : loss 0.3844526708126068 - f1-score (micro avg)  0.5808\n",
      "2022-06-20 12:54:33,379 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 12:54:33,430 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:54:37,287 epoch 2 - iter 29/290 - loss 0.53605886 - samples/sec: 30.20 - lr: 0.000005\n",
      "2022-06-20 12:54:41,100 epoch 2 - iter 58/290 - loss 0.45938035 - samples/sec: 30.48 - lr: 0.000005\n",
      "2022-06-20 12:54:44,892 epoch 2 - iter 87/290 - loss 0.46567017 - samples/sec: 30.79 - lr: 0.000005\n",
      "2022-06-20 12:54:48,718 epoch 2 - iter 116/290 - loss 0.45998796 - samples/sec: 30.45 - lr: 0.000005\n",
      "2022-06-20 12:54:52,523 epoch 2 - iter 145/290 - loss 0.44204603 - samples/sec: 30.60 - lr: 0.000005\n",
      "2022-06-20 12:54:56,415 epoch 2 - iter 174/290 - loss 0.43658454 - samples/sec: 29.93 - lr: 0.000005\n",
      "2022-06-20 12:55:00,261 epoch 2 - iter 203/290 - loss 0.41694991 - samples/sec: 30.29 - lr: 0.000005\n",
      "2022-06-20 12:55:04,004 epoch 2 - iter 232/290 - loss 0.42420963 - samples/sec: 31.13 - lr: 0.000005\n",
      "2022-06-20 12:55:07,767 epoch 2 - iter 261/290 - loss 0.40969073 - samples/sec: 30.89 - lr: 0.000005\n",
      "2022-06-20 12:55:11,494 epoch 2 - iter 290/290 - loss 0.40578733 - samples/sec: 31.26 - lr: 0.000004\n",
      "2022-06-20 12:55:11,518 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:55:11,539 EPOCH 2 done: loss 0.4058 - lr 0.0000044\n",
      "2022-06-20 12:55:13,317 DEV : loss 0.23002225160598755 - f1-score (micro avg)  0.7312\n",
      "2022-06-20 12:55:13,323 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 12:55:13,366 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:55:17,016 epoch 3 - iter 29/290 - loss 0.27064653 - samples/sec: 31.91 - lr: 0.000004\n",
      "2022-06-20 12:55:20,803 epoch 3 - iter 58/290 - loss 0.24180420 - samples/sec: 30.76 - lr: 0.000004\n",
      "2022-06-20 12:55:24,575 epoch 3 - iter 87/290 - loss 0.25824785 - samples/sec: 30.82 - lr: 0.000004\n",
      "2022-06-20 12:55:28,315 epoch 3 - iter 116/290 - loss 0.28616363 - samples/sec: 31.15 - lr: 0.000004\n",
      "2022-06-20 12:55:32,232 epoch 3 - iter 145/290 - loss 0.28142079 - samples/sec: 29.74 - lr: 0.000004\n",
      "2022-06-20 12:55:35,976 epoch 3 - iter 174/290 - loss 0.31068736 - samples/sec: 31.12 - lr: 0.000004\n",
      "2022-06-20 12:55:39,698 epoch 3 - iter 203/290 - loss 0.30115649 - samples/sec: 31.38 - lr: 0.000004\n",
      "2022-06-20 12:55:43,310 epoch 3 - iter 232/290 - loss 0.30038308 - samples/sec: 32.18 - lr: 0.000004\n",
      "2022-06-20 12:55:46,939 epoch 3 - iter 261/290 - loss 0.29369980 - samples/sec: 32.03 - lr: 0.000004\n",
      "2022-06-20 12:55:50,659 epoch 3 - iter 290/290 - loss 0.28271249 - samples/sec: 31.32 - lr: 0.000004\n",
      "2022-06-20 12:55:50,675 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:55:50,679 EPOCH 3 done: loss 0.2827 - lr 0.0000039\n",
      "2022-06-20 12:55:52,682 DEV : loss 0.2997819185256958 - f1-score (micro avg)  0.7754\n",
      "2022-06-20 12:55:52,697 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 12:55:52,716 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:55:56,529 epoch 4 - iter 29/290 - loss 0.19026010 - samples/sec: 30.48 - lr: 0.000004\n",
      "2022-06-20 12:56:00,214 epoch 4 - iter 58/290 - loss 0.20843844 - samples/sec: 31.61 - lr: 0.000004\n",
      "2022-06-20 12:56:04,004 epoch 4 - iter 87/290 - loss 0.21303915 - samples/sec: 30.75 - lr: 0.000004\n",
      "2022-06-20 12:56:07,684 epoch 4 - iter 116/290 - loss 0.19789433 - samples/sec: 31.65 - lr: 0.000004\n",
      "2022-06-20 12:56:11,435 epoch 4 - iter 145/290 - loss 0.19563189 - samples/sec: 31.06 - lr: 0.000004\n",
      "2022-06-20 12:56:15,214 epoch 4 - iter 174/290 - loss 0.20761429 - samples/sec: 30.90 - lr: 0.000004\n",
      "2022-06-20 12:56:18,948 epoch 4 - iter 203/290 - loss 0.22118790 - samples/sec: 31.20 - lr: 0.000003\n",
      "2022-06-20 12:56:22,765 epoch 4 - iter 232/290 - loss 0.22057045 - samples/sec: 30.51 - lr: 0.000003\n",
      "2022-06-20 12:56:26,514 epoch 4 - iter 261/290 - loss 0.23191487 - samples/sec: 31.13 - lr: 0.000003\n",
      "2022-06-20 12:56:30,170 epoch 4 - iter 290/290 - loss 0.23240021 - samples/sec: 31.95 - lr: 0.000003\n",
      "2022-06-20 12:56:30,186 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:56:30,216 EPOCH 4 done: loss 0.2324 - lr 0.0000033\n",
      "2022-06-20 12:56:32,057 DEV : loss 0.32403895258903503 - f1-score (micro avg)  0.8073\n",
      "2022-06-20 12:56:32,114 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 12:56:32,179 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:56:36,025 epoch 5 - iter 29/290 - loss 0.25232982 - samples/sec: 30.29 - lr: 0.000003\n",
      "2022-06-20 12:56:39,788 epoch 5 - iter 58/290 - loss 0.21513671 - samples/sec: 30.95 - lr: 0.000003\n",
      "2022-06-20 12:56:43,531 epoch 5 - iter 87/290 - loss 0.21653757 - samples/sec: 31.13 - lr: 0.000003\n",
      "2022-06-20 12:56:47,161 epoch 5 - iter 116/290 - loss 0.22450216 - samples/sec: 32.03 - lr: 0.000003\n",
      "2022-06-20 12:56:50,893 epoch 5 - iter 145/290 - loss 0.21296748 - samples/sec: 31.22 - lr: 0.000003\n",
      "2022-06-20 12:56:54,623 epoch 5 - iter 174/290 - loss 0.20101455 - samples/sec: 31.23 - lr: 0.000003\n",
      "2022-06-20 12:56:58,419 epoch 5 - iter 203/290 - loss 0.21916154 - samples/sec: 30.70 - lr: 0.000003\n",
      "2022-06-20 12:57:02,153 epoch 5 - iter 232/290 - loss 0.22390691 - samples/sec: 31.20 - lr: 0.000003\n",
      "2022-06-20 12:57:05,865 epoch 5 - iter 261/290 - loss 0.22366000 - samples/sec: 31.38 - lr: 0.000003\n",
      "2022-06-20 12:57:09,549 epoch 5 - iter 290/290 - loss 0.21944154 - samples/sec: 31.64 - lr: 0.000003\n",
      "2022-06-20 12:57:09,564 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:57:09,572 EPOCH 5 done: loss 0.2194 - lr 0.0000028\n",
      "2022-06-20 12:57:11,411 DEV : loss 0.2826554477214813 - f1-score (micro avg)  0.8145\n",
      "2022-06-20 12:57:11,426 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 12:57:11,467 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:57:15,293 epoch 6 - iter 29/290 - loss 0.24754013 - samples/sec: 30.44 - lr: 0.000003\n",
      "2022-06-20 12:57:19,260 epoch 6 - iter 58/290 - loss 0.21598232 - samples/sec: 29.58 - lr: 0.000003\n",
      "2022-06-20 12:57:22,998 epoch 6 - iter 87/290 - loss 0.21028699 - samples/sec: 31.16 - lr: 0.000003\n",
      "2022-06-20 12:57:26,716 epoch 6 - iter 116/290 - loss 0.19500586 - samples/sec: 31.33 - lr: 0.000003\n",
      "2022-06-20 12:57:30,467 epoch 6 - iter 145/290 - loss 0.17965735 - samples/sec: 31.06 - lr: 0.000003\n",
      "2022-06-20 12:57:34,122 epoch 6 - iter 174/290 - loss 0.17339285 - samples/sec: 31.89 - lr: 0.000002\n",
      "2022-06-20 12:57:37,819 epoch 6 - iter 203/290 - loss 0.17198302 - samples/sec: 31.50 - lr: 0.000002\n",
      "2022-06-20 12:57:41,530 epoch 6 - iter 232/290 - loss 0.17714836 - samples/sec: 31.39 - lr: 0.000002\n",
      "2022-06-20 12:57:45,236 epoch 6 - iter 261/290 - loss 0.17926501 - samples/sec: 31.44 - lr: 0.000002\n",
      "2022-06-20 12:57:49,017 epoch 6 - iter 290/290 - loss 0.18234074 - samples/sec: 30.80 - lr: 0.000002\n",
      "2022-06-20 12:57:49,032 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:57:49,036 EPOCH 6 done: loss 0.1823 - lr 0.0000022\n",
      "2022-06-20 12:57:50,875 DEV : loss 0.3347948491573334 - f1-score (micro avg)  0.8476\n",
      "2022-06-20 12:57:50,890 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 12:57:50,910 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:57:54,744 epoch 7 - iter 29/290 - loss 0.08721051 - samples/sec: 30.31 - lr: 0.000002\n",
      "2022-06-20 12:57:58,458 epoch 7 - iter 58/290 - loss 0.14115117 - samples/sec: 31.37 - lr: 0.000002\n",
      "2022-06-20 12:58:02,191 epoch 7 - iter 87/290 - loss 0.13784855 - samples/sec: 31.21 - lr: 0.000002\n",
      "2022-06-20 12:58:05,881 epoch 7 - iter 116/290 - loss 0.15412602 - samples/sec: 31.58 - lr: 0.000002\n",
      "2022-06-20 12:58:09,671 epoch 7 - iter 145/290 - loss 0.15197780 - samples/sec: 30.75 - lr: 0.000002\n",
      "2022-06-20 12:58:13,569 epoch 7 - iter 174/290 - loss 0.15231739 - samples/sec: 29.88 - lr: 0.000002\n",
      "2022-06-20 12:58:17,322 epoch 7 - iter 203/290 - loss 0.14987783 - samples/sec: 31.04 - lr: 0.000002\n",
      "2022-06-20 12:58:21,120 epoch 7 - iter 232/290 - loss 0.14954207 - samples/sec: 30.67 - lr: 0.000002\n",
      "2022-06-20 12:58:24,900 epoch 7 - iter 261/290 - loss 0.14376406 - samples/sec: 30.89 - lr: 0.000002\n",
      "2022-06-20 12:58:28,715 epoch 7 - iter 290/290 - loss 0.15147190 - samples/sec: 30.81 - lr: 0.000002\n",
      "2022-06-20 12:58:28,728 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:58:28,739 EPOCH 7 done: loss 0.1515 - lr 0.0000017\n",
      "2022-06-20 12:58:30,508 DEV : loss 0.39109310507774353 - f1-score (micro avg)  0.839\n",
      "2022-06-20 12:58:30,516 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 12:58:30,814 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:58:34,589 epoch 8 - iter 29/290 - loss 0.10496767 - samples/sec: 30.91 - lr: 0.000002\n",
      "2022-06-20 12:58:38,364 epoch 8 - iter 58/290 - loss 0.11617813 - samples/sec: 30.84 - lr: 0.000002\n",
      "2022-06-20 12:58:42,015 epoch 8 - iter 87/290 - loss 0.12318685 - samples/sec: 31.90 - lr: 0.000002\n",
      "2022-06-20 12:58:45,738 epoch 8 - iter 116/290 - loss 0.13086657 - samples/sec: 31.29 - lr: 0.000001\n",
      "2022-06-20 12:58:49,496 epoch 8 - iter 145/290 - loss 0.12686333 - samples/sec: 31.00 - lr: 0.000001\n",
      "2022-06-20 12:58:53,356 epoch 8 - iter 174/290 - loss 0.14264034 - samples/sec: 30.14 - lr: 0.000001\n",
      "2022-06-20 12:58:57,079 epoch 8 - iter 203/290 - loss 0.14192240 - samples/sec: 31.29 - lr: 0.000001\n",
      "2022-06-20 12:59:00,803 epoch 8 - iter 232/290 - loss 0.14132358 - samples/sec: 31.35 - lr: 0.000001\n",
      "2022-06-20 12:59:04,573 epoch 8 - iter 261/290 - loss 0.14698061 - samples/sec: 30.93 - lr: 0.000001\n",
      "2022-06-20 12:59:08,247 epoch 8 - iter 290/290 - loss 0.14496121 - samples/sec: 31.68 - lr: 0.000001\n",
      "2022-06-20 12:59:08,263 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:59:08,279 EPOCH 8 done: loss 0.1450 - lr 0.0000011\n",
      "2022-06-20 12:59:10,175 DEV : loss 0.3458554446697235 - f1-score (micro avg)  0.8433\n",
      "2022-06-20 12:59:10,190 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 12:59:10,212 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:59:13,997 epoch 9 - iter 29/290 - loss 0.10979111 - samples/sec: 30.85 - lr: 0.000001\n",
      "2022-06-20 12:59:17,772 epoch 9 - iter 58/290 - loss 0.11920734 - samples/sec: 30.87 - lr: 0.000001\n",
      "2022-06-20 12:59:21,558 epoch 9 - iter 87/290 - loss 0.12361143 - samples/sec: 30.77 - lr: 0.000001\n",
      "2022-06-20 12:59:25,334 epoch 9 - iter 116/290 - loss 0.12318347 - samples/sec: 30.95 - lr: 0.000001\n",
      "2022-06-20 12:59:29,167 epoch 9 - iter 145/290 - loss 0.11857958 - samples/sec: 30.37 - lr: 0.000001\n",
      "2022-06-20 12:59:32,995 epoch 9 - iter 174/290 - loss 0.11494829 - samples/sec: 30.44 - lr: 0.000001\n",
      "2022-06-20 12:59:36,752 epoch 9 - iter 203/290 - loss 0.11174190 - samples/sec: 31.16 - lr: 0.000001\n",
      "2022-06-20 12:59:40,511 epoch 9 - iter 232/290 - loss 0.11286404 - samples/sec: 30.92 - lr: 0.000001\n",
      "2022-06-20 12:59:44,226 epoch 9 - iter 261/290 - loss 0.11454326 - samples/sec: 31.35 - lr: 0.000001\n",
      "2022-06-20 12:59:47,973 epoch 9 - iter 290/290 - loss 0.11475108 - samples/sec: 31.10 - lr: 0.000001\n",
      "2022-06-20 12:59:47,988 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:59:47,991 EPOCH 9 done: loss 0.1148 - lr 0.0000006\n",
      "2022-06-20 12:59:49,761 DEV : loss 0.3378506898880005 - f1-score (micro avg)  0.8507\n",
      "2022-06-20 12:59:49,768 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 12:59:49,792 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 12:59:53,509 epoch 10 - iter 29/290 - loss 0.10938813 - samples/sec: 31.45 - lr: 0.000001\n",
      "2022-06-20 12:59:57,308 epoch 10 - iter 58/290 - loss 0.10637143 - samples/sec: 30.78 - lr: 0.000000\n",
      "2022-06-20 13:00:01,135 epoch 10 - iter 87/290 - loss 0.12130816 - samples/sec: 30.37 - lr: 0.000000\n",
      "2022-06-20 13:00:04,899 epoch 10 - iter 116/290 - loss 0.12360547 - samples/sec: 30.88 - lr: 0.000000\n",
      "2022-06-20 13:00:08,665 epoch 10 - iter 145/290 - loss 0.12173290 - samples/sec: 30.92 - lr: 0.000000\n",
      "2022-06-20 13:00:12,327 epoch 10 - iter 174/290 - loss 0.12485128 - samples/sec: 31.81 - lr: 0.000000\n",
      "2022-06-20 13:00:16,050 epoch 10 - iter 203/290 - loss 0.12406340 - samples/sec: 31.23 - lr: 0.000000\n",
      "2022-06-20 13:00:19,695 epoch 10 - iter 232/290 - loss 0.11998476 - samples/sec: 32.04 - lr: 0.000000\n",
      "2022-06-20 13:00:23,670 epoch 10 - iter 261/290 - loss 0.11749422 - samples/sec: 29.28 - lr: 0.000000\n",
      "2022-06-20 13:00:27,326 epoch 10 - iter 290/290 - loss 0.11619692 - samples/sec: 31.80 - lr: 0.000000\n",
      "2022-06-20 13:00:27,367 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 13:00:27,395 EPOCH 10 done: loss 0.1162 - lr 0.0000000\n",
      "2022-06-20 13:00:29,182 DEV : loss 0.3535381257534027 - f1-score (micro avg)  0.8315\n",
      "2022-06-20 13:00:29,218 BAD EPOCHS (no improvement): 4\n",
      "2022-06-20 13:01:09,397 ----------------------------------------------------------------------------------------------------\n",
      "2022-06-20 13:01:09,428 Testing using last state of model ...\n",
      "2022-06-20 13:01:22,457 0.77\t0.8264\t0.7972\t0.6628\n",
      "2022-06-20 13:01:22,467 \n",
      "Results:\n",
      "- F-score (micro) 0.7972\n",
      "- F-score (macro) 0.7972\n",
      "- Accuracy 0.6628\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CT     0.7700    0.8264    0.7972       968\n",
      "\n",
      "   micro avg     0.7700    0.8264    0.7972       968\n",
      "   macro avg     0.7700    0.8264    0.7972       968\n",
      "weighted avg     0.7700    0.8264    0.7972       968\n",
      " samples avg     0.6628    0.6628    0.6628       968\n",
      "\n",
      "2022-06-20 13:01:22,484 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.7972097658196312,\n",
       " 'dev_score_history': [0.5808383233532933,\n",
       "  0.7311827956989247,\n",
       "  0.7753623188405797,\n",
       "  0.8072727272727273,\n",
       "  0.8145454545454545,\n",
       "  0.8475836431226765,\n",
       "  0.8389513108614233,\n",
       "  0.8432835820895523,\n",
       "  0.8507462686567164,\n",
       "  0.8314606741573034],\n",
       " 'train_loss_history': [0.949919008878515,\n",
       "  0.40578732975968484,\n",
       "  0.28271249064705106,\n",
       "  0.2324002127852648,\n",
       "  0.2194415353511049,\n",
       "  0.18234073505706924,\n",
       "  0.15147190432366534,\n",
       "  0.14496121027377656,\n",
       "  0.11475107674469999,\n",
       "  0.11619692441975707],\n",
       " 'dev_loss_history': [tensor(0.3845, device='cuda:0'),\n",
       "  tensor(0.2300, device='cuda:0'),\n",
       "  tensor(0.2998, device='cuda:0'),\n",
       "  tensor(0.3240, device='cuda:0'),\n",
       "  tensor(0.2827, device='cuda:0'),\n",
       "  tensor(0.3348, device='cuda:0'),\n",
       "  tensor(0.3911, device='cuda:0'),\n",
       "  tensor(0.3459, device='cuda:0'),\n",
       "  tensor(0.3379, device='cuda:0'),\n",
       "  tensor(0.3535, device='cuda:0')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_type = 'ct'\n",
    "\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n",
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       )\n",
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type='ct',\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False,\n",
    "                        )\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. run fine-tuning\n",
    "trainer.fine_tune(model_folder,\n",
    "                  learning_rate=5.0e-6,\n",
    "                  mini_batch_size=4,\n",
    "                  #mini_batch_chunk_size=1,  # remove this parameter to speed up computation if you have a big GPU\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract targets from Reddit conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-19 21:30:29,281 loading file ../../../data-ceph/arguana/arg-generation/claim-target-tagger/model/final-model.pt\n",
      "2022-05-19 21:30:59,358 SequenceTagger predicts: Dictionary with 5 tags: O, S-CT, B-CT, E-CT, I-CT\n"
     ]
    }
   ],
   "source": [
    "from ca_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets_and_stances(df):\n",
    "    unique_conclusions = df.title.unique().tolist()\n",
    "    unique_conclusions_targets = extract_targets(unique_conclusions)\n",
    "    unique_conclusions_stances = get_stances(unique_conclusions_targets, unique_conclusions)\n",
    "\n",
    "    conc_to_targets = {x[0]: x[1] for x in zip(unique_conclusions, unique_conclusions_targets)}\n",
    "    conc_to_stances = {x[0]: x[1] for x in zip(unique_conclusions, unique_conclusions_stances)}\n",
    "    \n",
    "    df['conclusion_targets'] = df.title.apply(lambda x: conc_to_targets[x])\n",
    "    df['conclusion_stance']  = df.title.apply(lambda x: conc_to_stances[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 1997/1997 [00:33<00:00, 60.30it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract conclusion target and stances for dev_sample_all\n",
    "dev_df = pd.read_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/sample_valid_conclusion_all_preprocessed.pkl')\n",
    "\n",
    "dev_df = dev_df[dev_df.title.str.len() > 0]\n",
    "dev_df = extract_targets_and_stances(dev_df)\n",
    "dev_df.to_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/sample_valid_conclusion_all_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 2000/2000 [00:35<00:00, 56.97it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract conclusion target and stances for test_sample_all\n",
    "test_df = pd.read_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/sample_test_conclusion_all_preprocessed.pkl')\n",
    "\n",
    "test_df = test_df[test_df.title.str.len() > 0]\n",
    "test_df = extract_targets_and_stances(test_df)\n",
    "test_df.to_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/sample_test_conclusion_all_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 8519/8519 [02:21<00:00, 60.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract conclusion target and stances for test_all\n",
    "test_df = pd.read_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/test_conclusion_all_preprocessed.pkl')\n",
    "\n",
    "test_df = test_df[test_df.title.str.len() > 0]\n",
    "test_df = extract_targets_and_stances(test_df)\n",
    "test_df.to_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/test_conclusion_all_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8533"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>bart_conclusion</th>\n",
       "      <th>conclusion_targets</th>\n",
       "      <th>conclusion_stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>410850</th>\n",
       "      <td>people should come with instructions</td>\n",
       "      <td>i think people should be required by law to use a cheat sheet if they meet someone they</td>\n",
       "      <td>people should come with instructions</td>\n",
       "      <td>0.997129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410858</th>\n",
       "      <td>People should not be heavily criticized for things they put on social media in the distant past</td>\n",
       "      <td>i think the internet should stop being as harsh on people for things they put on social</td>\n",
       "      <td>distant past</td>\n",
       "      <td>-0.952858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410902</th>\n",
       "      <td>We shouldn't focus on slowing climate change</td>\n",
       "      <td>joint statement:: there are other environmental issues that are a greater problem for</td>\n",
       "      <td>focus on slowing climate change</td>\n",
       "      <td>-0.997431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410910</th>\n",
       "      <td>The Australian PM was right to tell students to stop activism around global warming</td>\n",
       "      <td>I believe that activism is a terrible way to combat climate change</td>\n",
       "      <td>stop activism around global warming</td>\n",
       "      <td>0.999497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410916</th>\n",
       "      <td>Feeding cats or dogs a diet with meat is indefensible.</td>\n",
       "      <td>if a cat or dog eats her life then it's a animal killer and they should be</td>\n",
       "      <td>Feeding cats or dogs a diet with meat</td>\n",
       "      <td>-0.984038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   title  \\\n",
       "410850                                                              people should come with instructions   \n",
       "410858   People should not be heavily criticized for things they put on social media in the distant past   \n",
       "410902                                                      We shouldn't focus on slowing climate change   \n",
       "410910               The Australian PM was right to tell students to stop activism around global warming   \n",
       "410916                                            Feeding cats or dogs a diet with meat is indefensible.   \n",
       "\n",
       "                                                                                bart_conclusion  \\\n",
       "410850  i think people should be required by law to use a cheat sheet if they meet someone they   \n",
       "410858  i think the internet should stop being as harsh on people for things they put on social   \n",
       "410902    joint statement:: there are other environmental issues that are a greater problem for   \n",
       "410910                       I believe that activism is a terrible way to combat climate change   \n",
       "410916               if a cat or dog eats her life then it's a animal killer and they should be   \n",
       "\n",
       "                           conclusion_targets  conclusion_stance  \n",
       "410850   people should come with instructions           0.997129  \n",
       "410858                           distant past          -0.952858  \n",
       "410902        focus on slowing climate change          -0.997431  \n",
       "410910    stop activism around global warming           0.999497  \n",
       "410916  Feeding cats or dogs a diet with meat          -0.984038  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[['title', 'bart_conclusion', 'conclusion_targets', 'conclusion_stance']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16043b4afea6df4cc9c8277bea4f74cd7012ce4985455d3fd8e496ab2325b686"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
