{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src-py/')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus    \n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_debater_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data-sample/claim-target-tagger/data/ibm_ds/'\n",
    "model_folder = '../data-sample/claim-target-tagger/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Target tagger on IBM dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-05 17:23:12,607 Reading data from ../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds\n",
      "2022-07-05 17:23:12,608 Train: ../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/train_ds.tsv\n",
      "2022-07-05 17:23:12,609 Dev: None\n",
      "2022-07-05 17:23:12,609 Test: ../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/test_ds.tsv\n"
     ]
    }
   ],
   "source": [
    "columns = {0: 'text', 1: 'pos', 2: 'ct'}\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train_ds.tsv',\n",
    "                              test_file='test_ds.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-05 17:23:14,256 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157/1157 [00:00<00:00, 31388.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-05 17:23:14,297 Corpus contains the labels: pos (#14127), ct (#14127)\n",
      "2022-07-05 17:23:14,297 Created (for label 'ct') Dictionary with 4 tags: <unk>, B-CT, I-CT, O\n",
      "Dictionary with 4 tags: <unk>, B-CT, I-CT, O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-05 17:23:34,900 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:23:34,926 Model: \"SequenceTagger(\n",
      "  (embeddings): TransformerWordEmbeddings(\n",
      "    (model): XLMRobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 1024)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (18): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (19): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (20): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (21): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (22): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (23): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (linear): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-07-05 17:23:34,942 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:23:34,947 Corpus: \"Corpus: 1157 train + 129 dev + 974 test sentences\"\n",
      "2022-07-05 17:23:34,951 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:23:34,955 Parameters:\n",
      "2022-07-05 17:23:34,960  - learning_rate: \"5e-06\"\n",
      "2022-07-05 17:23:34,964  - mini_batch_size: \"4\"\n",
      "2022-07-05 17:23:34,968  - patience: \"3\"\n",
      "2022-07-05 17:23:34,972  - anneal_factor: \"0.5\"\n",
      "2022-07-05 17:23:34,977  - max_epochs: \"10\"\n",
      "2022-07-05 17:23:34,981  - shuffle: \"True\"\n",
      "2022-07-05 17:23:34,986  - train_with_dev: \"False\"\n",
      "2022-07-05 17:23:34,996  - batch_growth_annealing: \"False\"\n",
      "2022-07-05 17:23:35,005 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:23:35,009 Model training base path: \"../../data-ceph/arguana/arg-generation/claim-target-tagger/model\"\n",
      "2022-07-05 17:23:35,013 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:23:35,025 Device: cuda:0\n",
      "2022-07-05 17:23:35,029 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:23:35,033 Embeddings storage mode: none\n",
      "2022-07-05 17:23:35,065 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:23:39,077 epoch 1 - iter 29/290 - loss 2.40882682 - samples/sec: 29.02 - lr: 0.000001\n",
      "2022-07-05 17:23:42,885 epoch 1 - iter 58/290 - loss 2.34567244 - samples/sec: 30.57 - lr: 0.000001\n",
      "2022-07-05 17:23:46,668 epoch 1 - iter 87/290 - loss 2.25777726 - samples/sec: 30.80 - lr: 0.000002\n",
      "2022-07-05 17:23:50,419 epoch 1 - iter 116/290 - loss 2.06170893 - samples/sec: 31.06 - lr: 0.000002\n",
      "2022-07-05 17:23:54,210 epoch 1 - iter 145/290 - loss 1.84478455 - samples/sec: 30.80 - lr: 0.000003\n",
      "2022-07-05 17:23:58,005 epoch 1 - iter 174/290 - loss 1.68191510 - samples/sec: 30.68 - lr: 0.000003\n",
      "2022-07-05 17:24:01,787 epoch 1 - iter 203/290 - loss 1.54952572 - samples/sec: 30.81 - lr: 0.000003\n",
      "2022-07-05 17:24:05,683 epoch 1 - iter 232/290 - loss 1.44582386 - samples/sec: 29.90 - lr: 0.000004\n",
      "2022-07-05 17:24:09,517 epoch 1 - iter 261/290 - loss 1.34587077 - samples/sec: 30.38 - lr: 0.000005\n",
      "2022-07-05 17:24:13,591 epoch 1 - iter 290/290 - loss 1.27132514 - samples/sec: 28.53 - lr: 0.000005\n",
      "2022-07-05 17:24:13,606 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:24:13,610 EPOCH 1 done: loss 1.2713 - lr 0.0000050\n",
      "2022-07-05 17:24:15,483 DEV : loss 0.42683786153793335 - f1-score (micro avg)  0.5688\n",
      "2022-07-05 17:24:15,498 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:24:15,517 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:24:19,371 epoch 2 - iter 29/290 - loss 0.45991240 - samples/sec: 30.15 - lr: 0.000005\n",
      "2022-07-05 17:24:23,076 epoch 2 - iter 58/290 - loss 0.49887884 - samples/sec: 31.45 - lr: 0.000005\n",
      "2022-07-05 17:24:26,903 epoch 2 - iter 87/290 - loss 0.47767639 - samples/sec: 30.44 - lr: 0.000005\n",
      "2022-07-05 17:24:30,670 epoch 2 - iter 116/290 - loss 0.46478147 - samples/sec: 30.93 - lr: 0.000005\n",
      "2022-07-05 17:24:34,477 epoch 2 - iter 145/290 - loss 0.46037867 - samples/sec: 30.60 - lr: 0.000005\n",
      "2022-07-05 17:24:38,250 epoch 2 - iter 174/290 - loss 0.44429528 - samples/sec: 30.81 - lr: 0.000005\n",
      "2022-07-05 17:24:42,137 epoch 2 - iter 203/290 - loss 0.43402239 - samples/sec: 29.96 - lr: 0.000005\n",
      "2022-07-05 17:24:45,850 epoch 2 - iter 232/290 - loss 0.42161986 - samples/sec: 31.30 - lr: 0.000005\n",
      "2022-07-05 17:24:49,538 epoch 2 - iter 261/290 - loss 0.41883260 - samples/sec: 31.59 - lr: 0.000005\n",
      "2022-07-05 17:24:53,324 epoch 2 - iter 290/290 - loss 0.40538378 - samples/sec: 30.70 - lr: 0.000004\n",
      "2022-07-05 17:24:53,341 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:24:53,345 EPOCH 2 done: loss 0.4054 - lr 0.0000044\n",
      "2022-07-05 17:24:55,186 DEV : loss 0.3864201605319977 - f1-score (micro avg)  0.679\n",
      "2022-07-05 17:24:55,202 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:24:55,233 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:24:58,975 epoch 3 - iter 29/290 - loss 0.29021121 - samples/sec: 31.13 - lr: 0.000004\n",
      "2022-07-05 17:25:02,802 epoch 3 - iter 58/290 - loss 0.29702488 - samples/sec: 30.43 - lr: 0.000004\n",
      "2022-07-05 17:25:06,683 epoch 3 - iter 87/290 - loss 0.28960833 - samples/sec: 30.03 - lr: 0.000004\n",
      "2022-07-05 17:25:10,522 epoch 3 - iter 116/290 - loss 0.28700924 - samples/sec: 30.34 - lr: 0.000004\n",
      "2022-07-05 17:25:14,316 epoch 3 - iter 145/290 - loss 0.30026489 - samples/sec: 30.71 - lr: 0.000004\n",
      "2022-07-05 17:25:18,091 epoch 3 - iter 174/290 - loss 0.29689663 - samples/sec: 30.86 - lr: 0.000004\n",
      "2022-07-05 17:25:21,900 epoch 3 - iter 203/290 - loss 0.30379373 - samples/sec: 30.58 - lr: 0.000004\n",
      "2022-07-05 17:25:25,718 epoch 3 - iter 232/290 - loss 0.28781133 - samples/sec: 30.51 - lr: 0.000004\n",
      "2022-07-05 17:25:29,559 epoch 3 - iter 261/290 - loss 0.28922468 - samples/sec: 30.33 - lr: 0.000004\n",
      "2022-07-05 17:25:33,302 epoch 3 - iter 290/290 - loss 0.28710730 - samples/sec: 31.06 - lr: 0.000004\n",
      "2022-07-05 17:25:33,317 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:25:33,321 EPOCH 3 done: loss 0.2871 - lr 0.0000039\n",
      "2022-07-05 17:25:35,353 DEV : loss 0.3938683867454529 - f1-score (micro avg)  0.7226\n",
      "2022-07-05 17:25:35,368 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:25:35,393 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:25:39,120 epoch 4 - iter 29/290 - loss 0.20399997 - samples/sec: 31.19 - lr: 0.000004\n",
      "2022-07-05 17:25:42,944 epoch 4 - iter 58/290 - loss 0.21489653 - samples/sec: 30.47 - lr: 0.000004\n",
      "2022-07-05 17:25:46,781 epoch 4 - iter 87/290 - loss 0.22422386 - samples/sec: 30.36 - lr: 0.000004\n",
      "2022-07-05 17:25:50,654 epoch 4 - iter 116/290 - loss 0.21094715 - samples/sec: 30.08 - lr: 0.000004\n",
      "2022-07-05 17:25:54,535 epoch 4 - iter 145/290 - loss 0.22943412 - samples/sec: 30.08 - lr: 0.000004\n",
      "2022-07-05 17:25:58,326 epoch 4 - iter 174/290 - loss 0.21633387 - samples/sec: 30.73 - lr: 0.000004\n",
      "2022-07-05 17:26:02,102 epoch 4 - iter 203/290 - loss 0.22826800 - samples/sec: 30.85 - lr: 0.000003\n",
      "2022-07-05 17:26:05,912 epoch 4 - iter 232/290 - loss 0.23850956 - samples/sec: 30.58 - lr: 0.000003\n",
      "2022-07-05 17:26:09,697 epoch 4 - iter 261/290 - loss 0.24377712 - samples/sec: 30.85 - lr: 0.000003\n",
      "2022-07-05 17:26:13,382 epoch 4 - iter 290/290 - loss 0.24888051 - samples/sec: 31.54 - lr: 0.000003\n",
      "2022-07-05 17:26:13,397 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:26:13,401 EPOCH 4 done: loss 0.2489 - lr 0.0000033\n",
      "2022-07-05 17:26:15,180 DEV : loss 0.5124596357345581 - f1-score (micro avg)  0.7481\n",
      "2022-07-05 17:26:15,196 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:26:15,236 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:26:19,141 epoch 5 - iter 29/290 - loss 0.17301933 - samples/sec: 29.83 - lr: 0.000003\n",
      "2022-07-05 17:26:22,956 epoch 5 - iter 58/290 - loss 0.17659494 - samples/sec: 30.53 - lr: 0.000003\n",
      "2022-07-05 17:26:26,771 epoch 5 - iter 87/290 - loss 0.16791438 - samples/sec: 30.53 - lr: 0.000003\n",
      "2022-07-05 17:26:30,547 epoch 5 - iter 116/290 - loss 0.19332115 - samples/sec: 30.85 - lr: 0.000003\n",
      "2022-07-05 17:26:34,377 epoch 5 - iter 145/290 - loss 0.19124036 - samples/sec: 30.41 - lr: 0.000003\n",
      "2022-07-05 17:26:38,212 epoch 5 - iter 174/290 - loss 0.19614350 - samples/sec: 30.38 - lr: 0.000003\n",
      "2022-07-05 17:26:41,983 epoch 5 - iter 203/290 - loss 0.19773009 - samples/sec: 30.88 - lr: 0.000003\n",
      "2022-07-05 17:26:45,774 epoch 5 - iter 232/290 - loss 0.20302049 - samples/sec: 30.73 - lr: 0.000003\n",
      "2022-07-05 17:26:49,458 epoch 5 - iter 261/290 - loss 0.20141957 - samples/sec: 31.56 - lr: 0.000003\n",
      "2022-07-05 17:26:53,231 epoch 5 - iter 290/290 - loss 0.20990759 - samples/sec: 30.87 - lr: 0.000003\n",
      "2022-07-05 17:26:53,247 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:26:53,251 EPOCH 5 done: loss 0.2099 - lr 0.0000028\n",
      "2022-07-05 17:26:55,043 DEV : loss 0.44996923208236694 - f1-score (micro avg)  0.7574\n",
      "2022-07-05 17:26:55,059 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:26:55,099 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:26:58,905 epoch 6 - iter 29/290 - loss 0.15537161 - samples/sec: 30.60 - lr: 0.000003\n",
      "2022-07-05 17:27:02,945 epoch 6 - iter 58/290 - loss 0.13029416 - samples/sec: 28.83 - lr: 0.000003\n",
      "2022-07-05 17:27:06,739 epoch 6 - iter 87/290 - loss 0.14362581 - samples/sec: 30.71 - lr: 0.000003\n",
      "2022-07-05 17:27:10,555 epoch 6 - iter 116/290 - loss 0.15811669 - samples/sec: 30.46 - lr: 0.000003\n",
      "2022-07-05 17:27:14,413 epoch 6 - iter 145/290 - loss 0.15705639 - samples/sec: 30.19 - lr: 0.000003\n",
      "2022-07-05 17:27:18,290 epoch 6 - iter 174/290 - loss 0.15884965 - samples/sec: 30.11 - lr: 0.000002\n",
      "2022-07-05 17:27:22,168 epoch 6 - iter 203/290 - loss 0.16072917 - samples/sec: 30.10 - lr: 0.000002\n",
      "2022-07-05 17:27:25,895 epoch 6 - iter 232/290 - loss 0.16020670 - samples/sec: 31.26 - lr: 0.000002\n",
      "2022-07-05 17:27:29,750 epoch 6 - iter 261/290 - loss 0.17146964 - samples/sec: 30.21 - lr: 0.000002\n",
      "2022-07-05 17:27:33,480 epoch 6 - iter 290/290 - loss 0.17178083 - samples/sec: 31.23 - lr: 0.000002\n",
      "2022-07-05 17:27:33,496 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:27:33,501 EPOCH 6 done: loss 0.1718 - lr 0.0000022\n",
      "2022-07-05 17:27:35,279 DEV : loss 0.5613431334495544 - f1-score (micro avg)  0.7681\n",
      "2022-07-05 17:27:35,294 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:27:35,318 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:27:39,029 epoch 7 - iter 29/290 - loss 0.12624017 - samples/sec: 31.32 - lr: 0.000002\n",
      "2022-07-05 17:27:42,897 epoch 7 - iter 58/290 - loss 0.16274421 - samples/sec: 30.06 - lr: 0.000002\n",
      "2022-07-05 17:27:46,753 epoch 7 - iter 87/290 - loss 0.17046979 - samples/sec: 30.21 - lr: 0.000002\n",
      "2022-07-05 17:27:50,528 epoch 7 - iter 116/290 - loss 0.16774254 - samples/sec: 30.79 - lr: 0.000002\n",
      "2022-07-05 17:27:54,371 epoch 7 - iter 145/290 - loss 0.16874923 - samples/sec: 30.24 - lr: 0.000002\n",
      "2022-07-05 17:27:58,181 epoch 7 - iter 174/290 - loss 0.16197720 - samples/sec: 30.58 - lr: 0.000002\n",
      "2022-07-05 17:28:01,951 epoch 7 - iter 203/290 - loss 0.15931644 - samples/sec: 30.90 - lr: 0.000002\n",
      "2022-07-05 17:28:05,720 epoch 7 - iter 232/290 - loss 0.15813802 - samples/sec: 30.90 - lr: 0.000002\n",
      "2022-07-05 17:28:09,584 epoch 7 - iter 261/290 - loss 0.16687024 - samples/sec: 30.14 - lr: 0.000002\n",
      "2022-07-05 17:28:13,401 epoch 7 - iter 290/290 - loss 0.16211894 - samples/sec: 30.53 - lr: 0.000002\n",
      "2022-07-05 17:28:13,417 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:28:13,421 EPOCH 7 done: loss 0.1621 - lr 0.0000017\n",
      "2022-07-05 17:28:15,198 DEV : loss 0.5354974269866943 - f1-score (micro avg)  0.7744\n",
      "2022-07-05 17:28:15,213 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:28:15,244 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:28:19,034 epoch 8 - iter 29/290 - loss 0.14605973 - samples/sec: 30.73 - lr: 0.000002\n",
      "2022-07-05 17:28:22,879 epoch 8 - iter 58/290 - loss 0.13683728 - samples/sec: 30.29 - lr: 0.000002\n",
      "2022-07-05 17:28:26,771 epoch 8 - iter 87/290 - loss 0.13835919 - samples/sec: 29.93 - lr: 0.000002\n",
      "2022-07-05 17:28:30,467 epoch 8 - iter 116/290 - loss 0.13596277 - samples/sec: 31.53 - lr: 0.000001\n",
      "2022-07-05 17:28:34,493 epoch 8 - iter 145/290 - loss 0.13538391 - samples/sec: 28.93 - lr: 0.000001\n",
      "2022-07-05 17:28:38,227 epoch 8 - iter 174/290 - loss 0.14134256 - samples/sec: 31.20 - lr: 0.000001\n",
      "2022-07-05 17:28:42,053 epoch 8 - iter 203/290 - loss 0.14875440 - samples/sec: 30.44 - lr: 0.000001\n",
      "2022-07-05 17:28:45,794 epoch 8 - iter 232/290 - loss 0.14400147 - samples/sec: 31.07 - lr: 0.000001\n",
      "2022-07-05 17:28:49,630 epoch 8 - iter 261/290 - loss 0.14395487 - samples/sec: 30.37 - lr: 0.000001\n",
      "2022-07-05 17:28:53,361 epoch 8 - iter 290/290 - loss 0.14497840 - samples/sec: 31.23 - lr: 0.000001\n",
      "2022-07-05 17:28:53,386 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:28:53,391 EPOCH 8 done: loss 0.1450 - lr 0.0000011\n",
      "2022-07-05 17:28:55,227 DEV : loss 0.6210302114486694 - f1-score (micro avg)  0.7739\n",
      "2022-07-05 17:28:55,243 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:28:55,275 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:28:59,101 epoch 9 - iter 29/290 - loss 0.17352826 - samples/sec: 30.44 - lr: 0.000001\n",
      "2022-07-05 17:29:02,857 epoch 9 - iter 58/290 - loss 0.14994002 - samples/sec: 31.02 - lr: 0.000001\n",
      "2022-07-05 17:29:06,718 epoch 9 - iter 87/290 - loss 0.13052350 - samples/sec: 30.16 - lr: 0.000001\n",
      "2022-07-05 17:29:10,673 epoch 9 - iter 116/290 - loss 0.13059258 - samples/sec: 29.45 - lr: 0.000001\n",
      "2022-07-05 17:29:14,542 epoch 9 - iter 145/290 - loss 0.13051567 - samples/sec: 30.12 - lr: 0.000001\n",
      "2022-07-05 17:29:18,320 epoch 9 - iter 174/290 - loss 0.12166398 - samples/sec: 30.83 - lr: 0.000001\n",
      "2022-07-05 17:29:22,183 epoch 9 - iter 203/290 - loss 0.12345086 - samples/sec: 30.15 - lr: 0.000001\n",
      "2022-07-05 17:29:25,996 epoch 9 - iter 232/290 - loss 0.11954414 - samples/sec: 30.55 - lr: 0.000001\n",
      "2022-07-05 17:29:29,830 epoch 9 - iter 261/290 - loss 0.11690895 - samples/sec: 30.39 - lr: 0.000001\n",
      "2022-07-05 17:29:33,594 epoch 9 - iter 290/290 - loss 0.11524690 - samples/sec: 30.94 - lr: 0.000001\n",
      "2022-07-05 17:29:33,610 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:29:33,614 EPOCH 9 done: loss 0.1152 - lr 0.0000006\n",
      "2022-07-05 17:29:35,397 DEV : loss 0.6892235279083252 - f1-score (micro avg)  0.7519\n",
      "2022-07-05 17:29:35,412 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:29:35,444 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:29:39,258 epoch 10 - iter 29/290 - loss 0.12589662 - samples/sec: 30.55 - lr: 0.000001\n",
      "2022-07-05 17:29:42,984 epoch 10 - iter 58/290 - loss 0.14052960 - samples/sec: 31.19 - lr: 0.000000\n",
      "2022-07-05 17:29:46,693 epoch 10 - iter 87/290 - loss 0.15441139 - samples/sec: 31.42 - lr: 0.000000\n",
      "2022-07-05 17:29:50,358 epoch 10 - iter 116/290 - loss 0.15099049 - samples/sec: 31.86 - lr: 0.000000\n",
      "2022-07-05 17:29:54,202 epoch 10 - iter 145/290 - loss 0.14979122 - samples/sec: 30.30 - lr: 0.000000\n",
      "2022-07-05 17:29:58,098 epoch 10 - iter 174/290 - loss 0.14411631 - samples/sec: 29.90 - lr: 0.000000\n",
      "2022-07-05 17:30:01,915 epoch 10 - iter 203/290 - loss 0.14341399 - samples/sec: 30.51 - lr: 0.000000\n",
      "2022-07-05 17:30:05,831 epoch 10 - iter 232/290 - loss 0.13976788 - samples/sec: 29.74 - lr: 0.000000\n",
      "2022-07-05 17:30:09,851 epoch 10 - iter 261/290 - loss 0.13647348 - samples/sec: 28.96 - lr: 0.000000\n",
      "2022-07-05 17:30:13,625 epoch 10 - iter 290/290 - loss 0.13723460 - samples/sec: 30.87 - lr: 0.000000\n",
      "2022-07-05 17:30:13,641 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:30:13,645 EPOCH 10 done: loss 0.1372 - lr 0.0000000\n",
      "2022-07-05 17:30:15,486 DEV : loss 0.7018179297447205 - f1-score (micro avg)  0.7576\n",
      "2022-07-05 17:30:15,494 BAD EPOCHS (no improvement): 4\n",
      "2022-07-05 17:30:32,618 ----------------------------------------------------------------------------------------------------\n",
      "2022-07-05 17:30:32,643 Testing using last state of model ...\n",
      "2022-07-05 17:30:45,815 0.7867\t0.8306\t0.808\t0.6779\n",
      "2022-07-05 17:30:45,838 \n",
      "Results:\n",
      "- F-score (micro) 0.808\n",
      "- F-score (macro) 0.808\n",
      "- Accuracy 0.6779\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CT     0.7867    0.8306    0.8080       968\n",
      "\n",
      "   micro avg     0.7867    0.8306    0.8080       968\n",
      "   macro avg     0.7867    0.8306    0.8080       968\n",
      "weighted avg     0.7867    0.8306    0.8080       968\n",
      " samples avg     0.6779    0.6779    0.6779       968\n",
      "\n",
      "2022-07-05 17:30:45,842 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.8080402010050252,\n",
       " 'dev_score_history': [0.5687500000000001,\n",
       "  0.6789667896678967,\n",
       "  0.7226277372262774,\n",
       "  0.7481481481481481,\n",
       "  0.7573529411764706,\n",
       "  0.7680608365019012,\n",
       "  0.7744360902255639,\n",
       "  0.7739463601532566,\n",
       "  0.7518796992481204,\n",
       "  0.7575757575757576],\n",
       " 'train_loss_history': [1.2713251365018412,\n",
       "  0.4053837836965884,\n",
       "  0.2871073011735839,\n",
       "  0.24888050605821097,\n",
       "  0.209907591208884,\n",
       "  0.17178082751801205,\n",
       "  0.16211893949897646,\n",
       "  0.14497840200862383,\n",
       "  0.11524690419061442,\n",
       "  0.13723460010312408],\n",
       " 'dev_loss_history': [tensor(0.4268, device='cuda:0'),\n",
       "  tensor(0.3864, device='cuda:0'),\n",
       "  tensor(0.3939, device='cuda:0'),\n",
       "  tensor(0.5125, device='cuda:0'),\n",
       "  tensor(0.4500, device='cuda:0'),\n",
       "  tensor(0.5613, device='cuda:0'),\n",
       "  tensor(0.5355, device='cuda:0'),\n",
       "  tensor(0.6210, device='cuda:0'),\n",
       "  tensor(0.6892, device='cuda:0'),\n",
       "  tensor(0.7018, device='cuda:0')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_type = 'ct'\n",
    "\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n",
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       )\n",
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type='ct',\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False,\n",
    "                        )\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. run fine-tuning\n",
    "trainer.fine_tune(model_folder,\n",
    "                  learning_rate=5.0e-6,\n",
    "                  mini_batch_size=4,\n",
    "                  #mini_batch_chunk_size=1,  # remove this parameter to speed up computation if you have a big GPU\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract targets from Reddit conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ca_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets_and_stances(df):\n",
    "    unique_conclusions = df.title.unique().tolist()\n",
    "    unique_conclusions_targets = extract_targets(unique_conclusions)\n",
    "    unique_conclusions_stances = get_stances(unique_conclusions_targets, unique_conclusions)\n",
    "\n",
    "    conc_to_targets = {x[0]: x[1] for x in zip(unique_conclusions, unique_conclusions_targets)}\n",
    "    conc_to_stances = {x[0]: x[1] for x in zip(unique_conclusions, unique_conclusions_stances)}\n",
    "    \n",
    "    df['conclusion_targets'] = df.title.apply(lambda x: conc_to_targets[x])\n",
    "    df['conclusion_stance']  = df.title.apply(lambda x: conc_to_stances[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 1997/1997 [00:33<00:00, 60.30it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_df = pd.read_pickle(data_path + '/valid_conclusion_all_preprocessed.pkl')\n",
    "\n",
    "dev_df = dev_df[dev_df.title.str.len() > 0]\n",
    "dev_df = extract_targets_and_stances(dev_df)\n",
    "dev_df.to_pickle(data_path + '/valid_conclusion_all_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 8519/8519 [02:21<00:00, 60.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract conclusion target and stances for test_all\n",
    "test_df = pd.read_pickle(data_path + '/test_conclusion_all_preprocessed.pkl')\n",
    "\n",
    "test_df = test_df[test_df.title.str.len() > 0]\n",
    "test_df = extract_targets_and_stances(test_df)\n",
    "test_df.to_pickle(data_path + '/test_conclusion_all_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>bart_conclusion</th>\n",
       "      <th>conclusion_targets</th>\n",
       "      <th>conclusion_stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>410850</th>\n",
       "      <td>people should come with instructions</td>\n",
       "      <td>i think people should be required by law to use a cheat sheet if they meet someone they</td>\n",
       "      <td>people should come with instructions</td>\n",
       "      <td>0.997129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410858</th>\n",
       "      <td>People should not be heavily criticized for things they put on social media in the distant past</td>\n",
       "      <td>i think the internet should stop being as harsh on people for things they put on social</td>\n",
       "      <td>distant past</td>\n",
       "      <td>-0.952858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410902</th>\n",
       "      <td>We shouldn't focus on slowing climate change</td>\n",
       "      <td>joint statement:: there are other environmental issues that are a greater problem for</td>\n",
       "      <td>focus on slowing climate change</td>\n",
       "      <td>-0.997431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410910</th>\n",
       "      <td>The Australian PM was right to tell students to stop activism around global warming</td>\n",
       "      <td>I believe that activism is a terrible way to combat climate change</td>\n",
       "      <td>stop activism around global warming</td>\n",
       "      <td>0.999497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410916</th>\n",
       "      <td>Feeding cats or dogs a diet with meat is indefensible.</td>\n",
       "      <td>if a cat or dog eats her life then it's a animal killer and they should be</td>\n",
       "      <td>Feeding cats or dogs a diet with meat</td>\n",
       "      <td>-0.984038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   title  \\\n",
       "410850                                                              people should come with instructions   \n",
       "410858   People should not be heavily criticized for things they put on social media in the distant past   \n",
       "410902                                                      We shouldn't focus on slowing climate change   \n",
       "410910               The Australian PM was right to tell students to stop activism around global warming   \n",
       "410916                                            Feeding cats or dogs a diet with meat is indefensible.   \n",
       "\n",
       "                                                                                bart_conclusion  \\\n",
       "410850  i think people should be required by law to use a cheat sheet if they meet someone they   \n",
       "410858  i think the internet should stop being as harsh on people for things they put on social   \n",
       "410902    joint statement:: there are other environmental issues that are a greater problem for   \n",
       "410910                       I believe that activism is a terrible way to combat climate change   \n",
       "410916               if a cat or dog eats her life then it's a animal killer and they should be   \n",
       "\n",
       "                           conclusion_targets  conclusion_stance  \n",
       "410850   people should come with instructions           0.997129  \n",
       "410858                           distant past          -0.952858  \n",
       "410902        focus on slowing climate change          -0.997431  \n",
       "410910    stop activism around global warming           0.999497  \n",
       "410916  Feeding cats or dogs a diet with meat          -0.984038  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[['title', 'bart_conclusion', 'conclusion_targets', 'conclusion_stance']].head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16043b4afea6df4cc9c8277bea4f74cd7012ce4985455d3fd8e496ab2325b686"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
