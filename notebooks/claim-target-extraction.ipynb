{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src-py/')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus    \n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_debater_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/'\n",
    "model_folder = '../../../data-ceph/arguana/arg-generation/claim-target-tagger/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Target tagger on IBM dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:29:04,885 Reading data from ../../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds\n",
      "2022-05-01 16:29:04,886 Train: ../../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/train_ds.tsv\n",
      "2022-05-01 16:29:04,887 Dev: None\n",
      "2022-05-01 16:29:04,888 Test: ../../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/test_ds.tsv\n"
     ]
    }
   ],
   "source": [
    "columns = {0: 'text', 1: 'pos', 2: 'ct'}\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train_ds.tsv',\n",
    "                              test_file='test_ds.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:29:07,968 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1157it [00:00, 50027.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:29:08,019 Dictionary created for label 'ct' with 2 values: CT (seen 1143 times)\n",
      "Dictionary with 2 tags: <unk>, CT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:29:21,188 SequenceTagger predicts: Dictionary with 5 tags: O, S-CT, B-CT, E-CT, I-CT\n",
      "2022-05-01 16:29:21,393 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/flair/trainers/trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  \"There should be no best model saved at epoch 1 except there \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:29:21,400 Model: \"SequenceTagger(\n",
      "  (embeddings): TransformerWordEmbeddings(\n",
      "    (model): XLMRobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 1024)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (18): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (19): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (20): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (21): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (22): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (23): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (linear): Linear(in_features=1024, out_features=5, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2022-05-01 16:29:21,400 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:29:21,401 Corpus: \"Corpus: 1157 train + 129 dev + 974 test sentences\"\n",
      "2022-05-01 16:29:21,402 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:29:21,403 Parameters:\n",
      "2022-05-01 16:29:21,404  - learning_rate: \"0.000005\"\n",
      "2022-05-01 16:29:21,405  - mini_batch_size: \"4\"\n",
      "2022-05-01 16:29:21,406  - patience: \"3\"\n",
      "2022-05-01 16:29:21,407  - anneal_factor: \"0.5\"\n",
      "2022-05-01 16:29:21,408  - max_epochs: \"10\"\n",
      "2022-05-01 16:29:21,409  - shuffle: \"True\"\n",
      "2022-05-01 16:29:21,410  - train_with_dev: \"False\"\n",
      "2022-05-01 16:29:21,411  - batch_growth_annealing: \"False\"\n",
      "2022-05-01 16:29:21,412 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:29:21,413 Model training base path: \"../../../data-ceph/arguana/arg-generation/claim-target-tagger/model\"\n",
      "2022-05-01 16:29:21,413 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:29:21,413 Device: cuda:0\n",
      "2022-05-01 16:29:21,414 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:29:21,416 Embeddings storage mode: none\n",
      "2022-05-01 16:29:21,418 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:29:25,351 epoch 1 - iter 29/290 - loss 1.39394131 - samples/sec: 29.52 - lr: 0.000001\n",
      "2022-05-01 16:29:29,227 epoch 1 - iter 58/290 - loss 1.40310463 - samples/sec: 29.94 - lr: 0.000001\n",
      "2022-05-01 16:29:33,058 epoch 1 - iter 87/290 - loss 1.38514563 - samples/sec: 30.30 - lr: 0.000002\n",
      "2022-05-01 16:29:37,103 epoch 1 - iter 116/290 - loss 1.33319037 - samples/sec: 28.69 - lr: 0.000002\n",
      "2022-05-01 16:29:40,975 epoch 1 - iter 145/290 - loss 1.30010563 - samples/sec: 29.99 - lr: 0.000003\n",
      "2022-05-01 16:29:44,808 epoch 1 - iter 174/290 - loss 1.27206073 - samples/sec: 30.28 - lr: 0.000003\n",
      "2022-05-01 16:29:48,636 epoch 1 - iter 203/290 - loss 1.23715957 - samples/sec: 30.32 - lr: 0.000003\n",
      "2022-05-01 16:29:52,438 epoch 1 - iter 232/290 - loss 1.20594613 - samples/sec: 30.53 - lr: 0.000004\n",
      "2022-05-01 16:29:56,340 epoch 1 - iter 261/290 - loss 1.17031670 - samples/sec: 29.75 - lr: 0.000005\n",
      "2022-05-01 16:30:00,001 epoch 1 - iter 290/290 - loss 1.14075765 - samples/sec: 31.71 - lr: 0.000005\n",
      "2022-05-01 16:30:00,003 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:30:00,004 EPOCH 1 done: loss 1.1408 - lr 0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 17.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:30:02,137 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:30:02,149 DEV : loss 0.6870161890983582 - f1-score (micro avg)  0.0\n",
      "2022-05-01 16:30:02,152 BAD EPOCHS (no improvement): 4\n",
      "2022-05-01 16:30:02,153 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:30:05,930 epoch 2 - iter 29/290 - loss 0.80676257 - samples/sec: 30.74 - lr: 0.000005\n",
      "2022-05-01 16:30:09,804 epoch 2 - iter 58/290 - loss 0.76814420 - samples/sec: 29.96 - lr: 0.000005\n",
      "2022-05-01 16:30:13,724 epoch 2 - iter 87/290 - loss 0.74564540 - samples/sec: 29.61 - lr: 0.000005\n",
      "2022-05-01 16:30:17,645 epoch 2 - iter 116/290 - loss 0.74784677 - samples/sec: 29.60 - lr: 0.000005\n",
      "2022-05-01 16:30:21,523 epoch 2 - iter 145/290 - loss 0.73762055 - samples/sec: 29.93 - lr: 0.000005\n",
      "2022-05-01 16:30:25,326 epoch 2 - iter 174/290 - loss 0.71779486 - samples/sec: 30.52 - lr: 0.000005\n",
      "2022-05-01 16:30:29,162 epoch 2 - iter 203/290 - loss 0.69554402 - samples/sec: 30.26 - lr: 0.000005\n",
      "2022-05-01 16:30:33,494 epoch 2 - iter 232/290 - loss 0.67864590 - samples/sec: 26.79 - lr: 0.000005\n",
      "2022-05-01 16:30:37,368 epoch 2 - iter 261/290 - loss 0.66419520 - samples/sec: 29.96 - lr: 0.000005\n",
      "2022-05-01 16:30:41,139 epoch 2 - iter 290/290 - loss 0.64128527 - samples/sec: 30.79 - lr: 0.000004\n",
      "2022-05-01 16:30:41,142 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:30:41,142 EPOCH 2 done: loss 0.6413 - lr 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 17.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:30:43,047 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:30:43,057 DEV : loss 0.49243220686912537 - f1-score (micro avg)  0.6783\n",
      "2022-05-01 16:30:43,060 BAD EPOCHS (no improvement): 4\n",
      "2022-05-01 16:30:43,061 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:30:46,908 epoch 3 - iter 29/290 - loss 0.37728256 - samples/sec: 30.20 - lr: 0.000004\n",
      "2022-05-01 16:30:50,771 epoch 3 - iter 58/290 - loss 0.36272528 - samples/sec: 30.04 - lr: 0.000004\n",
      "2022-05-01 16:30:54,606 epoch 3 - iter 87/290 - loss 0.39249552 - samples/sec: 30.28 - lr: 0.000004\n",
      "2022-05-01 16:30:58,439 epoch 3 - iter 116/290 - loss 0.38959920 - samples/sec: 30.28 - lr: 0.000004\n",
      "2022-05-01 16:31:02,235 epoch 3 - iter 145/290 - loss 0.39415008 - samples/sec: 30.58 - lr: 0.000004\n",
      "2022-05-01 16:31:06,100 epoch 3 - iter 174/290 - loss 0.40498847 - samples/sec: 30.04 - lr: 0.000004\n",
      "2022-05-01 16:31:09,948 epoch 3 - iter 203/290 - loss 0.39318432 - samples/sec: 30.16 - lr: 0.000004\n",
      "2022-05-01 16:31:13,729 epoch 3 - iter 232/290 - loss 0.39040536 - samples/sec: 30.70 - lr: 0.000004\n",
      "2022-05-01 16:31:17,583 epoch 3 - iter 261/290 - loss 0.38170420 - samples/sec: 30.12 - lr: 0.000004\n",
      "2022-05-01 16:31:21,369 epoch 3 - iter 290/290 - loss 0.37915503 - samples/sec: 30.66 - lr: 0.000004\n",
      "2022-05-01 16:31:21,371 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:31:21,371 EPOCH 3 done: loss 0.3792 - lr 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 17.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:31:23,306 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:31:23,315 DEV : loss 0.3301384150981903 - f1-score (micro avg)  0.7088\n",
      "2022-05-01 16:31:23,318 BAD EPOCHS (no improvement): 4\n",
      "2022-05-01 16:31:23,319 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:31:27,755 epoch 4 - iter 29/290 - loss 0.31406910 - samples/sec: 26.17 - lr: 0.000004\n",
      "2022-05-01 16:31:31,537 epoch 4 - iter 58/290 - loss 0.29301066 - samples/sec: 30.69 - lr: 0.000004\n",
      "2022-05-01 16:31:35,260 epoch 4 - iter 87/290 - loss 0.29689222 - samples/sec: 31.18 - lr: 0.000004\n",
      "2022-05-01 16:31:39,174 epoch 4 - iter 116/290 - loss 0.30550631 - samples/sec: 29.65 - lr: 0.000004\n",
      "2022-05-01 16:31:43,026 epoch 4 - iter 145/290 - loss 0.28137027 - samples/sec: 30.14 - lr: 0.000004\n",
      "2022-05-01 16:31:46,908 epoch 4 - iter 174/290 - loss 0.28374901 - samples/sec: 29.90 - lr: 0.000004\n",
      "2022-05-01 16:31:50,712 epoch 4 - iter 203/290 - loss 0.29041878 - samples/sec: 30.51 - lr: 0.000003\n",
      "2022-05-01 16:31:54,542 epoch 4 - iter 232/290 - loss 0.29081964 - samples/sec: 30.31 - lr: 0.000003\n",
      "2022-05-01 16:31:58,300 epoch 4 - iter 261/290 - loss 0.28833174 - samples/sec: 30.88 - lr: 0.000003\n",
      "2022-05-01 16:32:02,123 epoch 4 - iter 290/290 - loss 0.28093797 - samples/sec: 30.36 - lr: 0.000003\n",
      "2022-05-01 16:32:02,125 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:32:02,126 EPOCH 4 done: loss 0.2809 - lr 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 17.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:32:04,090 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:32:04,100 DEV : loss 0.4771294891834259 - f1-score (micro avg)  0.7312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:32:04,104 BAD EPOCHS (no improvement): 4\n",
      "2022-05-01 16:32:04,104 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:32:08,036 epoch 5 - iter 29/290 - loss 0.28865009 - samples/sec: 29.53 - lr: 0.000003\n",
      "2022-05-01 16:32:11,884 epoch 5 - iter 58/290 - loss 0.28721383 - samples/sec: 30.16 - lr: 0.000003\n",
      "2022-05-01 16:32:15,726 epoch 5 - iter 87/290 - loss 0.25325380 - samples/sec: 30.21 - lr: 0.000003\n",
      "2022-05-01 16:32:20,053 epoch 5 - iter 116/290 - loss 0.25687301 - samples/sec: 26.83 - lr: 0.000003\n",
      "2022-05-01 16:32:23,814 epoch 5 - iter 145/290 - loss 0.25714270 - samples/sec: 30.86 - lr: 0.000003\n",
      "2022-05-01 16:32:27,590 epoch 5 - iter 174/290 - loss 0.24267253 - samples/sec: 30.74 - lr: 0.000003\n",
      "2022-05-01 16:32:31,389 epoch 5 - iter 203/290 - loss 0.23505762 - samples/sec: 30.56 - lr: 0.000003\n",
      "2022-05-01 16:32:35,251 epoch 5 - iter 232/290 - loss 0.23301916 - samples/sec: 30.05 - lr: 0.000003\n",
      "2022-05-01 16:32:39,088 epoch 5 - iter 261/290 - loss 0.23611423 - samples/sec: 30.26 - lr: 0.000003\n",
      "2022-05-01 16:32:42,853 epoch 5 - iter 290/290 - loss 0.24066043 - samples/sec: 30.83 - lr: 0.000003\n",
      "2022-05-01 16:32:42,856 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:32:42,856 EPOCH 5 done: loss 0.2407 - lr 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:32:44,892 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:32:44,901 DEV : loss 0.5390682816505432 - f1-score (micro avg)  0.7372\n",
      "2022-05-01 16:32:44,906 BAD EPOCHS (no improvement): 4\n",
      "2022-05-01 16:32:44,906 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:32:48,765 epoch 6 - iter 29/290 - loss 0.15000575 - samples/sec: 30.09 - lr: 0.000003\n",
      "2022-05-01 16:32:52,654 epoch 6 - iter 58/290 - loss 0.24351116 - samples/sec: 29.84 - lr: 0.000003\n",
      "2022-05-01 16:32:56,523 epoch 6 - iter 87/290 - loss 0.21480192 - samples/sec: 30.00 - lr: 0.000003\n",
      "2022-05-01 16:33:00,365 epoch 6 - iter 116/290 - loss 0.21280186 - samples/sec: 30.21 - lr: 0.000003\n",
      "2022-05-01 16:33:04,224 epoch 6 - iter 145/290 - loss 0.24328920 - samples/sec: 30.08 - lr: 0.000003\n",
      "2022-05-01 16:33:07,990 epoch 6 - iter 174/290 - loss 0.23137018 - samples/sec: 30.82 - lr: 0.000002\n",
      "2022-05-01 16:33:11,783 epoch 6 - iter 203/290 - loss 0.23270854 - samples/sec: 30.60 - lr: 0.000002\n",
      "2022-05-01 16:33:15,570 epoch 6 - iter 232/290 - loss 0.22598474 - samples/sec: 30.66 - lr: 0.000002\n",
      "2022-05-01 16:33:19,406 epoch 6 - iter 261/290 - loss 0.22634452 - samples/sec: 30.27 - lr: 0.000002\n",
      "2022-05-01 16:33:23,291 epoch 6 - iter 290/290 - loss 0.22712329 - samples/sec: 29.87 - lr: 0.000002\n",
      "2022-05-01 16:33:23,294 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:33:23,294 EPOCH 6 done: loss 0.2271 - lr 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:02<00:00, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:33:25,749 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:33:25,758 DEV : loss 0.5501019358634949 - f1-score (micro avg)  0.7636\n",
      "2022-05-01 16:33:25,762 BAD EPOCHS (no improvement): 4\n",
      "2022-05-01 16:33:25,762 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:33:29,551 epoch 7 - iter 29/290 - loss 0.14697487 - samples/sec: 30.65 - lr: 0.000002\n",
      "2022-05-01 16:33:33,482 epoch 7 - iter 58/290 - loss 0.15002215 - samples/sec: 29.53 - lr: 0.000002\n",
      "2022-05-01 16:33:37,306 epoch 7 - iter 87/290 - loss 0.15058176 - samples/sec: 30.35 - lr: 0.000002\n",
      "2022-05-01 16:33:41,134 epoch 7 - iter 116/290 - loss 0.15386545 - samples/sec: 30.33 - lr: 0.000002\n",
      "2022-05-01 16:33:44,832 epoch 7 - iter 145/290 - loss 0.14683373 - samples/sec: 31.39 - lr: 0.000002\n",
      "2022-05-01 16:33:48,652 epoch 7 - iter 174/290 - loss 0.16276921 - samples/sec: 30.39 - lr: 0.000002\n",
      "2022-05-01 16:33:52,501 epoch 7 - iter 203/290 - loss 0.16921189 - samples/sec: 30.16 - lr: 0.000002\n",
      "2022-05-01 16:33:56,399 epoch 7 - iter 232/290 - loss 0.17172012 - samples/sec: 29.78 - lr: 0.000002\n",
      "2022-05-01 16:34:00,224 epoch 7 - iter 261/290 - loss 0.18184380 - samples/sec: 30.34 - lr: 0.000002\n",
      "2022-05-01 16:34:04,110 epoch 7 - iter 290/290 - loss 0.17782460 - samples/sec: 29.87 - lr: 0.000002\n",
      "2022-05-01 16:34:04,112 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:34:04,113 EPOCH 7 done: loss 0.1778 - lr 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 17.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:34:06,052 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:34:06,062 DEV : loss 0.5791816711425781 - f1-score (micro avg)  0.7647\n",
      "2022-05-01 16:34:06,067 BAD EPOCHS (no improvement): 4\n",
      "2022-05-01 16:34:06,067 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:34:09,905 epoch 8 - iter 29/290 - loss 0.19255112 - samples/sec: 30.26 - lr: 0.000002\n",
      "2022-05-01 16:34:13,693 epoch 8 - iter 58/290 - loss 0.19081356 - samples/sec: 30.64 - lr: 0.000002\n",
      "2022-05-01 16:34:17,966 epoch 8 - iter 87/290 - loss 0.18303466 - samples/sec: 27.16 - lr: 0.000002\n",
      "2022-05-01 16:34:21,739 epoch 8 - iter 116/290 - loss 0.19049393 - samples/sec: 30.76 - lr: 0.000001\n",
      "2022-05-01 16:34:25,538 epoch 8 - iter 145/290 - loss 0.18064759 - samples/sec: 30.55 - lr: 0.000001\n",
      "2022-05-01 16:34:29,328 epoch 8 - iter 174/290 - loss 0.18161132 - samples/sec: 30.63 - lr: 0.000001\n",
      "2022-05-01 16:34:33,125 epoch 8 - iter 203/290 - loss 0.17904067 - samples/sec: 30.57 - lr: 0.000001\n",
      "2022-05-01 16:34:36,858 epoch 8 - iter 232/290 - loss 0.17729747 - samples/sec: 31.10 - lr: 0.000001\n",
      "2022-05-01 16:34:40,592 epoch 8 - iter 261/290 - loss 0.17546824 - samples/sec: 31.08 - lr: 0.000001\n",
      "2022-05-01 16:34:44,422 epoch 8 - iter 290/290 - loss 0.17970192 - samples/sec: 30.31 - lr: 0.000001\n",
      "2022-05-01 16:34:44,424 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:34:44,425 EPOCH 8 done: loss 0.1797 - lr 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:34:46,413 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:34:46,422 DEV : loss 0.511544406414032 - f1-score (micro avg)  0.7794\n",
      "2022-05-01 16:34:46,426 BAD EPOCHS (no improvement): 4\n",
      "2022-05-01 16:34:46,427 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:34:50,208 epoch 9 - iter 29/290 - loss 0.28618755 - samples/sec: 30.71 - lr: 0.000001\n",
      "2022-05-01 16:34:54,100 epoch 9 - iter 58/290 - loss 0.21005557 - samples/sec: 29.82 - lr: 0.000001\n",
      "2022-05-01 16:34:57,932 epoch 9 - iter 87/290 - loss 0.18516733 - samples/sec: 30.29 - lr: 0.000001\n",
      "2022-05-01 16:35:01,765 epoch 9 - iter 116/290 - loss 0.17993167 - samples/sec: 30.29 - lr: 0.000001\n",
      "2022-05-01 16:35:05,513 epoch 9 - iter 145/290 - loss 0.17693395 - samples/sec: 30.97 - lr: 0.000001\n",
      "2022-05-01 16:35:09,250 epoch 9 - iter 174/290 - loss 0.17303323 - samples/sec: 31.07 - lr: 0.000001\n",
      "2022-05-01 16:35:13,004 epoch 9 - iter 203/290 - loss 0.16733077 - samples/sec: 30.92 - lr: 0.000001\n",
      "2022-05-01 16:35:16,739 epoch 9 - iter 232/290 - loss 0.16071606 - samples/sec: 31.07 - lr: 0.000001\n",
      "2022-05-01 16:35:20,623 epoch 9 - iter 261/290 - loss 0.16594342 - samples/sec: 29.88 - lr: 0.000001\n",
      "2022-05-01 16:35:24,393 epoch 9 - iter 290/290 - loss 0.16248182 - samples/sec: 30.79 - lr: 0.000001\n",
      "2022-05-01 16:35:24,395 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:35:24,395 EPOCH 9 done: loss 0.1625 - lr 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:02<00:00, 13.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:35:27,044 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:35:27,054 DEV : loss 0.5512567162513733 - f1-score (micro avg)  0.7849\n",
      "2022-05-01 16:35:27,058 BAD EPOCHS (no improvement): 4\n",
      "2022-05-01 16:35:27,058 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:35:30,841 epoch 10 - iter 29/290 - loss 0.11398264 - samples/sec: 30.69 - lr: 0.000001\n",
      "2022-05-01 16:35:34,646 epoch 10 - iter 58/290 - loss 0.10573766 - samples/sec: 30.51 - lr: 0.000000\n",
      "2022-05-01 16:35:38,443 epoch 10 - iter 87/290 - loss 0.10254649 - samples/sec: 30.57 - lr: 0.000000\n",
      "2022-05-01 16:35:42,185 epoch 10 - iter 116/290 - loss 0.12466263 - samples/sec: 31.04 - lr: 0.000000\n",
      "2022-05-01 16:35:46,054 epoch 10 - iter 145/290 - loss 0.13699229 - samples/sec: 30.01 - lr: 0.000000\n",
      "2022-05-01 16:35:49,856 epoch 10 - iter 174/290 - loss 0.13197114 - samples/sec: 30.54 - lr: 0.000000\n",
      "2022-05-01 16:35:53,694 epoch 10 - iter 203/290 - loss 0.13514002 - samples/sec: 30.24 - lr: 0.000000\n",
      "2022-05-01 16:35:57,560 epoch 10 - iter 232/290 - loss 0.13838521 - samples/sec: 30.02 - lr: 0.000000\n",
      "2022-05-01 16:36:01,417 epoch 10 - iter 261/290 - loss 0.13523708 - samples/sec: 30.10 - lr: 0.000000\n",
      "2022-05-01 16:36:05,274 epoch 10 - iter 290/290 - loss 0.13792176 - samples/sec: 30.09 - lr: 0.000000\n",
      "2022-05-01 16:36:05,277 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:36:05,277 EPOCH 10 done: loss 0.1379 - lr 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 17.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:36:07,213 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:36:07,222 DEV : loss 0.5474958419799805 - f1-score (micro avg)  0.782\n",
      "2022-05-01 16:36:07,226 BAD EPOCHS (no improvement): 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:36:10,049 ----------------------------------------------------------------------------------------------------\n",
      "2022-05-01 16:36:10,051 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:15<00:00, 15.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:36:25,758 Evaluating as a multi-label problem: False\n",
      "2022-05-01 16:36:25,769 0.7622\t0.8079\t0.7844\t0.6452\n",
      "2022-05-01 16:36:25,770 \n",
      "Results:\n",
      "- F-score (micro) 0.7844\n",
      "- F-score (macro) 0.7844\n",
      "- Accuracy 0.6452\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CT     0.7622    0.8079    0.7844       968\n",
      "\n",
      "   micro avg     0.7622    0.8079    0.7844       968\n",
      "   macro avg     0.7622    0.8079    0.7844       968\n",
      "weighted avg     0.7622    0.8079    0.7844       968\n",
      "\n",
      "2022-05-01 16:36:25,770 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.7843530591775327,\n",
       " 'dev_score_history': [0.0,\n",
       "  0.6783216783216782,\n",
       "  0.7087719298245615,\n",
       "  0.7311827956989249,\n",
       "  0.7372262773722628,\n",
       "  0.7636363636363634,\n",
       "  0.7647058823529411,\n",
       "  0.7794117647058822,\n",
       "  0.7849056603773584,\n",
       "  0.7819548872180451],\n",
       " 'train_loss_history': [1.1407576502377446,\n",
       "  0.6412852737494023,\n",
       "  0.3791550337100714,\n",
       "  0.2809379669566499,\n",
       "  0.24066042675218985,\n",
       "  0.22712329009040827,\n",
       "  0.17782459732661007,\n",
       "  0.17970192214440647,\n",
       "  0.16248181861427086,\n",
       "  0.1379217608272952],\n",
       " 'dev_loss_history': [0.6870161890983582,\n",
       "  0.49243220686912537,\n",
       "  0.3301384150981903,\n",
       "  0.4771294891834259,\n",
       "  0.5390682816505432,\n",
       "  0.5501019358634949,\n",
       "  0.5791816711425781,\n",
       "  0.511544406414032,\n",
       "  0.5512567162513733,\n",
       "  0.5474958419799805]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_type = 'ct'\n",
    "\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n",
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       )\n",
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type='ct',\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False,\n",
    "                        )\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. run fine-tuning\n",
    "trainer.fine_tune(model_folder,\n",
    "                  learning_rate=5.0e-6,\n",
    "                  mini_batch_size=4,\n",
    "                  #mini_batch_chunk_size=1,  # remove this parameter to speed up computation if you have a big GPU\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract targets from Reddit conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 16:37:45,833 loading file ../../../data-ceph/arguana/arg-generation/claim-target-tagger/model/final-model.pt\n",
      "2022-05-01 16:37:53,798 SequenceTagger predicts: Dictionary with 5 tags: O, S-CT, B-CT, E-CT, I-CT\n"
     ]
    }
   ],
   "source": [
    "# predict tags for sentences\n",
    "model = SequenceTagger.load(model_folder+'/final-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets(model, claims):\n",
    "    sentences = [Sentence(x) for x in claims]\n",
    "    model.predict(sentences)\n",
    "    # iterate through sentences and print predicted labels\n",
    "    targets = []\n",
    "    for sentence in sentences:\n",
    "        target_spans = sorted([(s.text, s.score) for s in sentence.get_spans('ct')], key=lambda x: -x[1])\n",
    "        if len(target_spans) > 0:\n",
    "            targets.append(target_spans[0][0])\n",
    "        else:\n",
    "            targets.append(sentence.to_original_text())\n",
    "        \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets_and_stances(df):\n",
    "    unique_conclusions = df.title.unique().tolist()\n",
    "    unique_conclusions_targets = extract_targets(model, unique_conclusions)\n",
    "    unique_conclusions_stances = get_stances(unique_conclusions_targets, unique_conclusions)\n",
    "\n",
    "    conc_to_targets = {x[0]: x[1] for x in zip(unique_conclusions, unique_conclusions_targets)}\n",
    "    conc_to_stances = {x[0]: x[1] for x in zip(unique_conclusions, unique_conclusions_stances)}\n",
    "    \n",
    "    df['conclusion_targets'] = df.title.apply(lambda x: conc_to_targets[x])\n",
    "    df['conclusion_stance']  = df.title.apply(lambda x: conc_to_stances[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 1997/1997 [00:33<00:00, 60.30it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract conclusion target and stances for dev_sample_all\n",
    "dev_df = pd.read_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/sample_valid_conclusion_all_preprocessed.pkl')\n",
    "\n",
    "dev_df = dev_df[dev_df.title.str.len() > 0]\n",
    "dev_df = extract_targets_and_stances(dev_df)\n",
    "dev_df.to_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/sample_valid_conclusion_all_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 2000/2000 [00:35<00:00, 56.97it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract conclusion target and stances for test_sample_all\n",
    "test_df = pd.read_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/sample_test_conclusion_all_preprocessed.pkl')\n",
    "\n",
    "test_df = test_df[test_df.title.str.len() > 0]\n",
    "test_df = extract_targets_and_stances(test_df)\n",
    "test_df.to_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/sample_test_conclusion_all_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16043b4afea6df4cc9c8277bea4f74cd7012ce4985455d3fd8e496ab2325b686"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
