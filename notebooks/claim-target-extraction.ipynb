{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src-py/')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus    \n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_debater_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/'\n",
    "model_folder = '../../../data-ceph/arguana/arg-generation/claim-target-tagger/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Target tagger on IBM dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 14:48:21,655 Reading data from ../../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds\n",
      "2022-01-07 14:48:21,656 Train: ../../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/train.tsv\n",
      "2022-01-07 14:48:21,657 Dev: None\n",
      "2022-01-07 14:48:21,658 Test: ../../../data-ceph/arguana/arg-generation/claim-target-tagger/data/ibm_ds/test.tsv\n"
     ]
    }
   ],
   "source": [
    "columns = {0: 'text', 1: 'pos', 2: 'ct'}\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train.tsv',\n",
    "                              test_file='test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 15:01:45,191 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 877/877 [00:00<00:00, 19214.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 15:01:45,240 Corpus contains the labels: pos (#11355), ct (#11355)\n",
      "2022-01-07 15:01:45,240 Created (for label 'ct') Dictionary with 4 tags: <unk>, O, B-CT, I-CT\n",
      "Dictionary with 4 tags: <unk>, O, B-CT, I-CT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 15:01:54,759 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:01:54,791 Model: \"SequenceTagger(\n",
      "  (embeddings): TransformerWordEmbeddings(\n",
      "    (model): XLMRobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 1024)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (18): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (19): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (20): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (21): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (22): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (23): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (linear): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-01-07 15:01:54,792 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:01:54,793 Corpus: \"Corpus: 877 train + 97 dev + 1286 test sentences\"\n",
      "2022-01-07 15:01:54,794 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:01:54,795 Parameters:\n",
      "2022-01-07 15:01:54,796  - learning_rate: \"5e-06\"\n",
      "2022-01-07 15:01:54,797  - mini_batch_size: \"4\"\n",
      "2022-01-07 15:01:54,798  - patience: \"3\"\n",
      "2022-01-07 15:01:54,799  - anneal_factor: \"0.5\"\n",
      "2022-01-07 15:01:54,799  - max_epochs: \"10\"\n",
      "2022-01-07 15:01:54,800  - shuffle: \"True\"\n",
      "2022-01-07 15:01:54,801  - train_with_dev: \"False\"\n",
      "2022-01-07 15:01:54,801  - batch_growth_annealing: \"False\"\n",
      "2022-01-07 15:01:54,802 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:01:54,803 Model training base path: \"../../../data-ceph/arguana/arg-generation/claim-target-tagger/model\"\n",
      "2022-01-07 15:01:54,803 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:01:54,804 Device: cuda:0\n",
      "2022-01-07 15:01:54,805 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:01:54,806 Embeddings storage mode: none\n",
      "2022-01-07 15:01:54,816 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:01:57,845 epoch 1 - iter 22/220 - loss 1.61152154 - samples/sec: 29.08 - lr: 0.000001\n",
      "2022-01-07 15:02:00,822 epoch 1 - iter 44/220 - loss 1.60376328 - samples/sec: 29.58 - lr: 0.000001\n",
      "2022-01-07 15:02:03,709 epoch 1 - iter 66/220 - loss 1.55892886 - samples/sec: 30.50 - lr: 0.000002\n",
      "2022-01-07 15:02:06,619 epoch 1 - iter 88/220 - loss 1.48334404 - samples/sec: 30.26 - lr: 0.000002\n",
      "2022-01-07 15:02:09,629 epoch 1 - iter 110/220 - loss 1.38347704 - samples/sec: 29.26 - lr: 0.000003\n",
      "2022-01-07 15:02:12,532 epoch 1 - iter 132/220 - loss 1.27787011 - samples/sec: 30.33 - lr: 0.000003\n",
      "2022-01-07 15:02:15,390 epoch 1 - iter 154/220 - loss 1.19825137 - samples/sec: 30.81 - lr: 0.000003\n",
      "2022-01-07 15:02:18,339 epoch 1 - iter 176/220 - loss 1.14518831 - samples/sec: 29.86 - lr: 0.000004\n",
      "2022-01-07 15:02:21,265 epoch 1 - iter 198/220 - loss 1.09042466 - samples/sec: 30.09 - lr: 0.000005\n",
      "2022-01-07 15:02:24,142 epoch 1 - iter 220/220 - loss 1.04262359 - samples/sec: 30.61 - lr: 0.000005\n",
      "2022-01-07 15:02:24,145 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:02:24,145 EPOCH 1 done: loss 1.0426 - lr 0.0000050\n",
      "2022-01-07 15:02:25,547 DEV : loss 0.3533633351325989 - f1-score (micro avg)  0.5333\n",
      "2022-01-07 15:02:25,548 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:02:25,549 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:02:28,469 epoch 2 - iter 22/220 - loss 0.50341005 - samples/sec: 30.19 - lr: 0.000005\n",
      "2022-01-07 15:02:31,351 epoch 2 - iter 44/220 - loss 0.49428164 - samples/sec: 30.54 - lr: 0.000005\n",
      "2022-01-07 15:02:34,326 epoch 2 - iter 66/220 - loss 0.47973693 - samples/sec: 29.60 - lr: 0.000005\n",
      "2022-01-07 15:02:37,215 epoch 2 - iter 88/220 - loss 0.46667246 - samples/sec: 30.48 - lr: 0.000005\n",
      "2022-01-07 15:02:40,183 epoch 2 - iter 110/220 - loss 0.44840202 - samples/sec: 29.67 - lr: 0.000005\n",
      "2022-01-07 15:02:43,104 epoch 2 - iter 132/220 - loss 0.43659157 - samples/sec: 30.15 - lr: 0.000005\n",
      "2022-01-07 15:02:46,029 epoch 2 - iter 154/220 - loss 0.43794867 - samples/sec: 30.10 - lr: 0.000005\n",
      "2022-01-07 15:02:49,053 epoch 2 - iter 176/220 - loss 0.43520846 - samples/sec: 29.13 - lr: 0.000005\n",
      "2022-01-07 15:02:51,951 epoch 2 - iter 198/220 - loss 0.43189092 - samples/sec: 30.38 - lr: 0.000005\n",
      "2022-01-07 15:02:54,844 epoch 2 - iter 220/220 - loss 0.42617399 - samples/sec: 30.44 - lr: 0.000004\n",
      "2022-01-07 15:02:54,846 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:02:54,847 EPOCH 2 done: loss 0.4262 - lr 0.0000044\n",
      "2022-01-07 15:02:56,249 DEV : loss 0.17149633169174194 - f1-score (micro avg)  0.7453\n",
      "2022-01-07 15:02:56,250 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:02:56,253 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:02:59,173 epoch 3 - iter 22/220 - loss 0.37826676 - samples/sec: 30.17 - lr: 0.000004\n",
      "2022-01-07 15:03:02,218 epoch 3 - iter 44/220 - loss 0.34297188 - samples/sec: 28.94 - lr: 0.000004\n",
      "2022-01-07 15:03:05,110 epoch 3 - iter 66/220 - loss 0.30071032 - samples/sec: 30.45 - lr: 0.000004\n",
      "2022-01-07 15:03:08,083 epoch 3 - iter 88/220 - loss 0.30202809 - samples/sec: 29.62 - lr: 0.000004\n",
      "2022-01-07 15:03:11,100 epoch 3 - iter 110/220 - loss 0.29789203 - samples/sec: 29.19 - lr: 0.000004\n",
      "2022-01-07 15:03:14,004 epoch 3 - iter 132/220 - loss 0.29576982 - samples/sec: 30.32 - lr: 0.000004\n",
      "2022-01-07 15:03:16,936 epoch 3 - iter 154/220 - loss 0.29582206 - samples/sec: 30.03 - lr: 0.000004\n",
      "2022-01-07 15:03:19,880 epoch 3 - iter 176/220 - loss 0.29743510 - samples/sec: 29.92 - lr: 0.000004\n",
      "2022-01-07 15:03:22,800 epoch 3 - iter 198/220 - loss 0.29314052 - samples/sec: 30.15 - lr: 0.000004\n",
      "2022-01-07 15:03:25,733 epoch 3 - iter 220/220 - loss 0.29372392 - samples/sec: 30.02 - lr: 0.000004\n",
      "2022-01-07 15:03:25,736 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:03:25,736 EPOCH 3 done: loss 0.2937 - lr 0.0000039\n",
      "2022-01-07 15:03:27,219 DEV : loss 0.17299453914165497 - f1-score (micro avg)  0.7902\n",
      "2022-01-07 15:03:27,221 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:03:27,267 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:03:30,281 epoch 4 - iter 22/220 - loss 0.19903934 - samples/sec: 29.24 - lr: 0.000004\n",
      "2022-01-07 15:03:33,197 epoch 4 - iter 44/220 - loss 0.20518423 - samples/sec: 30.19 - lr: 0.000004\n",
      "2022-01-07 15:03:36,115 epoch 4 - iter 66/220 - loss 0.20925682 - samples/sec: 30.18 - lr: 0.000004\n",
      "2022-01-07 15:03:39,021 epoch 4 - iter 88/220 - loss 0.22723101 - samples/sec: 30.30 - lr: 0.000004\n",
      "2022-01-07 15:03:42,186 epoch 4 - iter 110/220 - loss 0.23092879 - samples/sec: 27.82 - lr: 0.000004\n",
      "2022-01-07 15:03:45,016 epoch 4 - iter 132/220 - loss 0.22508379 - samples/sec: 31.11 - lr: 0.000004\n",
      "2022-01-07 15:03:47,947 epoch 4 - iter 154/220 - loss 0.23269028 - samples/sec: 30.05 - lr: 0.000003\n",
      "2022-01-07 15:03:50,811 epoch 4 - iter 176/220 - loss 0.23048010 - samples/sec: 30.74 - lr: 0.000003\n",
      "2022-01-07 15:03:53,792 epoch 4 - iter 198/220 - loss 0.22657335 - samples/sec: 29.54 - lr: 0.000003\n",
      "2022-01-07 15:03:56,720 epoch 4 - iter 220/220 - loss 0.22604856 - samples/sec: 30.08 - lr: 0.000003\n",
      "2022-01-07 15:03:56,722 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:03:56,723 EPOCH 4 done: loss 0.2260 - lr 0.0000033\n",
      "2022-01-07 15:03:58,203 DEV : loss 0.23873122036457062 - f1-score (micro avg)  0.8351\n",
      "2022-01-07 15:03:58,204 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:03:58,205 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:04:01,087 epoch 5 - iter 22/220 - loss 0.22832556 - samples/sec: 30.58 - lr: 0.000003\n",
      "2022-01-07 15:04:03,976 epoch 5 - iter 44/220 - loss 0.19100924 - samples/sec: 30.48 - lr: 0.000003\n",
      "2022-01-07 15:04:06,883 epoch 5 - iter 66/220 - loss 0.17856074 - samples/sec: 30.29 - lr: 0.000003\n",
      "2022-01-07 15:04:09,747 epoch 5 - iter 88/220 - loss 0.17873808 - samples/sec: 30.74 - lr: 0.000003\n",
      "2022-01-07 15:04:12,693 epoch 5 - iter 110/220 - loss 0.19163251 - samples/sec: 29.89 - lr: 0.000003\n",
      "2022-01-07 15:04:15,596 epoch 5 - iter 132/220 - loss 0.19022898 - samples/sec: 30.34 - lr: 0.000003\n",
      "2022-01-07 15:04:18,455 epoch 5 - iter 154/220 - loss 0.18854118 - samples/sec: 30.80 - lr: 0.000003\n",
      "2022-01-07 15:04:21,476 epoch 5 - iter 176/220 - loss 0.19714468 - samples/sec: 29.15 - lr: 0.000003\n",
      "2022-01-07 15:04:24,387 epoch 5 - iter 198/220 - loss 0.20060333 - samples/sec: 30.24 - lr: 0.000003\n",
      "2022-01-07 15:04:27,373 epoch 5 - iter 220/220 - loss 0.20374771 - samples/sec: 29.49 - lr: 0.000003\n",
      "2022-01-07 15:04:27,376 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:04:27,376 EPOCH 5 done: loss 0.2037 - lr 0.0000028\n",
      "2022-01-07 15:04:28,862 DEV : loss 0.20161832869052887 - f1-score (micro avg)  0.8776\n",
      "2022-01-07 15:04:28,863 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:04:28,864 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:04:31,776 epoch 6 - iter 22/220 - loss 0.21835127 - samples/sec: 30.26 - lr: 0.000003\n",
      "2022-01-07 15:04:34,568 epoch 6 - iter 44/220 - loss 0.20533553 - samples/sec: 31.55 - lr: 0.000003\n",
      "2022-01-07 15:04:37,428 epoch 6 - iter 66/220 - loss 0.20564951 - samples/sec: 30.79 - lr: 0.000003\n",
      "2022-01-07 15:04:40,331 epoch 6 - iter 88/220 - loss 0.18710142 - samples/sec: 30.33 - lr: 0.000003\n",
      "2022-01-07 15:04:43,236 epoch 6 - iter 110/220 - loss 0.18455035 - samples/sec: 30.31 - lr: 0.000003\n",
      "2022-01-07 15:04:46,237 epoch 6 - iter 132/220 - loss 0.17691381 - samples/sec: 29.35 - lr: 0.000002\n",
      "2022-01-07 15:04:49,133 epoch 6 - iter 154/220 - loss 0.17527242 - samples/sec: 30.41 - lr: 0.000002\n",
      "2022-01-07 15:04:52,003 epoch 6 - iter 176/220 - loss 0.17631408 - samples/sec: 30.68 - lr: 0.000002\n",
      "2022-01-07 15:04:54,964 epoch 6 - iter 198/220 - loss 0.18736390 - samples/sec: 29.74 - lr: 0.000002\n",
      "2022-01-07 15:04:57,885 epoch 6 - iter 220/220 - loss 0.18255921 - samples/sec: 30.15 - lr: 0.000002\n",
      "2022-01-07 15:04:57,887 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:04:57,888 EPOCH 6 done: loss 0.1826 - lr 0.0000022\n",
      "2022-01-07 15:04:59,309 DEV : loss 0.24843883514404297 - f1-score (micro avg)  0.8731\n",
      "2022-01-07 15:04:59,310 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:04:59,312 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:05:02,211 epoch 7 - iter 22/220 - loss 0.11455786 - samples/sec: 30.39 - lr: 0.000002\n",
      "2022-01-07 15:05:05,116 epoch 7 - iter 44/220 - loss 0.15029110 - samples/sec: 30.30 - lr: 0.000002\n",
      "2022-01-07 15:05:08,135 epoch 7 - iter 66/220 - loss 0.14590137 - samples/sec: 29.17 - lr: 0.000002\n",
      "2022-01-07 15:05:11,043 epoch 7 - iter 88/220 - loss 0.13798985 - samples/sec: 30.28 - lr: 0.000002\n",
      "2022-01-07 15:05:13,900 epoch 7 - iter 110/220 - loss 0.13476166 - samples/sec: 30.82 - lr: 0.000002\n",
      "2022-01-07 15:05:16,809 epoch 7 - iter 132/220 - loss 0.14007047 - samples/sec: 30.27 - lr: 0.000002\n",
      "2022-01-07 15:05:19,747 epoch 7 - iter 154/220 - loss 0.14206131 - samples/sec: 29.97 - lr: 0.000002\n",
      "2022-01-07 15:05:22,593 epoch 7 - iter 176/220 - loss 0.14225517 - samples/sec: 30.95 - lr: 0.000002\n",
      "2022-01-07 15:05:25,619 epoch 7 - iter 198/220 - loss 0.14115676 - samples/sec: 29.09 - lr: 0.000002\n",
      "2022-01-07 15:05:28,485 epoch 7 - iter 220/220 - loss 0.14026932 - samples/sec: 30.73 - lr: 0.000002\n",
      "2022-01-07 15:05:28,488 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:05:28,489 EPOCH 7 done: loss 0.1403 - lr 0.0000017\n",
      "2022-01-07 15:05:29,986 DEV : loss 0.2894372344017029 - f1-score (micro avg)  0.8821\n",
      "2022-01-07 15:05:29,987 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:05:29,991 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:05:32,984 epoch 8 - iter 22/220 - loss 0.22770725 - samples/sec: 29.44 - lr: 0.000002\n",
      "2022-01-07 15:05:35,984 epoch 8 - iter 44/220 - loss 0.17439621 - samples/sec: 29.35 - lr: 0.000002\n",
      "2022-01-07 15:05:38,904 epoch 8 - iter 66/220 - loss 0.16232075 - samples/sec: 30.16 - lr: 0.000002\n",
      "2022-01-07 15:05:41,769 epoch 8 - iter 88/220 - loss 0.16877426 - samples/sec: 30.74 - lr: 0.000001\n",
      "2022-01-07 15:05:44,572 epoch 8 - iter 110/220 - loss 0.15681594 - samples/sec: 31.41 - lr: 0.000001\n",
      "2022-01-07 15:05:47,340 epoch 8 - iter 132/220 - loss 0.15106895 - samples/sec: 31.81 - lr: 0.000001\n",
      "2022-01-07 15:05:50,301 epoch 8 - iter 154/220 - loss 0.15160562 - samples/sec: 29.74 - lr: 0.000001\n",
      "2022-01-07 15:05:53,295 epoch 8 - iter 176/220 - loss 0.14486917 - samples/sec: 29.41 - lr: 0.000001\n",
      "2022-01-07 15:05:56,135 epoch 8 - iter 198/220 - loss 0.14322670 - samples/sec: 31.00 - lr: 0.000001\n",
      "2022-01-07 15:05:58,934 epoch 8 - iter 220/220 - loss 0.14063570 - samples/sec: 31.47 - lr: 0.000001\n",
      "2022-01-07 15:05:58,936 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:05:58,937 EPOCH 8 done: loss 0.1406 - lr 0.0000011\n",
      "2022-01-07 15:06:00,347 DEV : loss 0.2741124629974365 - f1-score (micro avg)  0.8832\n",
      "2022-01-07 15:06:00,348 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:06:00,349 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:06:03,200 epoch 9 - iter 22/220 - loss 0.12250696 - samples/sec: 30.92 - lr: 0.000001\n",
      "2022-01-07 15:06:06,155 epoch 9 - iter 44/220 - loss 0.09642747 - samples/sec: 29.80 - lr: 0.000001\n",
      "2022-01-07 15:06:09,114 epoch 9 - iter 66/220 - loss 0.12354834 - samples/sec: 29.76 - lr: 0.000001\n",
      "2022-01-07 15:06:12,063 epoch 9 - iter 88/220 - loss 0.12272660 - samples/sec: 29.86 - lr: 0.000001\n",
      "2022-01-07 15:06:14,946 epoch 9 - iter 110/220 - loss 0.12112889 - samples/sec: 30.53 - lr: 0.000001\n",
      "2022-01-07 15:06:17,866 epoch 9 - iter 132/220 - loss 0.11458734 - samples/sec: 30.16 - lr: 0.000001\n",
      "2022-01-07 15:06:20,783 epoch 9 - iter 154/220 - loss 0.11652353 - samples/sec: 30.18 - lr: 0.000001\n",
      "2022-01-07 15:06:23,879 epoch 9 - iter 176/220 - loss 0.11414688 - samples/sec: 28.45 - lr: 0.000001\n",
      "2022-01-07 15:06:26,716 epoch 9 - iter 198/220 - loss 0.11539068 - samples/sec: 31.04 - lr: 0.000001\n",
      "2022-01-07 15:06:29,580 epoch 9 - iter 220/220 - loss 0.11979448 - samples/sec: 30.74 - lr: 0.000001\n",
      "2022-01-07 15:06:29,583 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:06:29,584 EPOCH 9 done: loss 0.1198 - lr 0.0000006\n",
      "2022-01-07 15:06:30,988 DEV : loss 0.2802909314632416 - f1-score (micro avg)  0.8788\n",
      "2022-01-07 15:06:30,990 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:06:30,991 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:06:33,895 epoch 10 - iter 22/220 - loss 0.06506051 - samples/sec: 30.35 - lr: 0.000001\n",
      "2022-01-07 15:06:36,791 epoch 10 - iter 44/220 - loss 0.11261709 - samples/sec: 30.41 - lr: 0.000000\n",
      "2022-01-07 15:06:39,713 epoch 10 - iter 66/220 - loss 0.09966948 - samples/sec: 30.13 - lr: 0.000000\n",
      "2022-01-07 15:06:42,591 epoch 10 - iter 88/220 - loss 0.10502251 - samples/sec: 30.60 - lr: 0.000000\n",
      "2022-01-07 15:06:45,574 epoch 10 - iter 110/220 - loss 0.10657653 - samples/sec: 29.52 - lr: 0.000000\n",
      "2022-01-07 15:06:48,511 epoch 10 - iter 132/220 - loss 0.10210362 - samples/sec: 29.98 - lr: 0.000000\n",
      "2022-01-07 15:06:51,365 epoch 10 - iter 154/220 - loss 0.10385642 - samples/sec: 30.85 - lr: 0.000000\n",
      "2022-01-07 15:06:54,242 epoch 10 - iter 176/220 - loss 0.10287933 - samples/sec: 30.61 - lr: 0.000000\n",
      "2022-01-07 15:06:57,204 epoch 10 - iter 198/220 - loss 0.10136229 - samples/sec: 29.72 - lr: 0.000000\n",
      "2022-01-07 15:07:00,019 epoch 10 - iter 220/220 - loss 0.10413014 - samples/sec: 31.28 - lr: 0.000000\n",
      "2022-01-07 15:07:00,022 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:07:00,022 EPOCH 10 done: loss 0.1041 - lr 0.0000000\n",
      "2022-01-07 15:07:01,549 DEV : loss 0.29758408665657043 - f1-score (micro avg)  0.8821\n",
      "2022-01-07 15:07:01,550 BAD EPOCHS (no improvement): 4\n",
      "2022-01-07 15:07:04,092 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-07 15:07:04,095 Testing using last state of model ...\n",
      "2022-01-07 15:07:22,103 0.7496\t0.8063\t0.7769\t0.6352\n",
      "2022-01-07 15:07:22,103 \n",
      "Results:\n",
      "- F-score (micro) 0.7769\n",
      "- F-score (macro) 0.7769\n",
      "- Accuracy 0.6352\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CT     0.7496    0.8063    0.7769      1270\n",
      "\n",
      "   micro avg     0.7496    0.8063    0.7769      1270\n",
      "   macro avg     0.7496    0.8063    0.7769      1270\n",
      "weighted avg     0.7496    0.8063    0.7769      1270\n",
      " samples avg     0.6352    0.6352    0.6352      1270\n",
      "\n",
      "2022-01-07 15:07:22,104 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.7769347496206374,\n",
       " 'dev_score_history': [0.5333333333333333,\n",
       "  0.7452830188679246,\n",
       "  0.7902439024390244,\n",
       "  0.8350515463917525,\n",
       "  0.8775510204081632,\n",
       "  0.8730964467005077,\n",
       "  0.8820512820512821,\n",
       "  0.8832487309644671,\n",
       "  0.8787878787878788,\n",
       "  0.8820512820512821],\n",
       " 'train_loss_history': [1.0426235850715049,\n",
       "  0.42617399000822315,\n",
       "  0.29372392421475935,\n",
       "  0.2260485619470602,\n",
       "  0.20374770659286134,\n",
       "  0.18255921371144995,\n",
       "  0.14026931725029088,\n",
       "  0.14063569918816818,\n",
       "  0.11979448386688905,\n",
       "  0.10413014359435109],\n",
       " 'dev_loss_history': [tensor(0.3534, device='cuda:0'),\n",
       "  tensor(0.1715, device='cuda:0'),\n",
       "  tensor(0.1730, device='cuda:0'),\n",
       "  tensor(0.2387, device='cuda:0'),\n",
       "  tensor(0.2016, device='cuda:0'),\n",
       "  tensor(0.2484, device='cuda:0'),\n",
       "  tensor(0.2894, device='cuda:0'),\n",
       "  tensor(0.2741, device='cuda:0'),\n",
       "  tensor(0.2803, device='cuda:0'),\n",
       "  tensor(0.2976, device='cuda:0')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_type = 'ct'\n",
    "\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n",
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       )\n",
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type='ct',\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False,\n",
    "                        )\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. run fine-tuning\n",
    "trainer.fine_tune(model_folder,\n",
    "                  learning_rate=5.0e-6,\n",
    "                  mini_batch_size=4,\n",
    "                  #mini_batch_chunk_size=1,  # remove this parameter to speed up computation if you have a big GPU\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract targets from Reddit conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets(claims):\n",
    "    sentences = [Sentence(x) for x in claims]\n",
    "    # predict tags for sentences\n",
    "    model = SequenceTagger.load(model_folder+'/final-model.pt')\n",
    "    model.predict(sentences)\n",
    "\n",
    "    # iterate through sentences and print predicted labels\n",
    "    targets = []\n",
    "    for sentence in sentences:\n",
    "        target_spans = sorted([(s.text, s.score) for s in sentence.get_spans('ct')], key=lambda x: -x[1])\n",
    "        if len(target_spans) > 0:\n",
    "            targets.append(target_spans[0][0])\n",
    "        else:\n",
    "            targets.append(sentence.to_original_text())\n",
    "        \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets_and_stances(df):\n",
    "    unique_conclusions = df.title.unique().tolist()\n",
    "    unique_conclusions_targets = extract_targets(unique_conclusions)\n",
    "    unique_conclusions_stances = get_stances(unique_conclusions_targets, unique_conclusions)\n",
    "\n",
    "    conc_to_targets = {x[0]: x[1] for x in zip(unique_conclusions, unique_conclusions_targets)}\n",
    "    conc_to_stances = {x[0]: x[1] for x in zip(unique_conclusions, unique_conclusions_stances)}\n",
    "    \n",
    "    df['conclusion_targets'] = df.title.apply(lambda x: conc_to_targets[x])\n",
    "    df['conclusion_stance']  = df.title.apply(lambda x: conc_to_stances[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract conclusion target and stances for dev_sample\n",
    "dev_df = pd.read_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/valid_conclusion_comp_remove_75sem_perc.pkl')\n",
    "dev_df = extract_targets_and_stances(dev_df)\n",
    "dev_df.to_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/valid_conclusion_comp_remove_75sem_perc_with_targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-17 16:40:46,107 loading file ../../../data-ceph/arguana/arg-generation/claim-target-tagger/model/final-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 2336/2336 [00:39<00:00, 74.02it/s]"
     ]
    }
   ],
   "source": [
    "#Extract conclusion target and stances for test_sample\n",
    "test_df = pd.read_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/test_concusion_comp_remove_75sem_perc_sample.pkl')\n",
    "test_df = extract_targets_and_stances(test_df)\n",
    "test_df.to_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/test_conclusion_comp_remove_75sem_perc_with_targets.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-17 16:48:21,201 loading file ../../../data-ceph/arguana/arg-generation/claim-target-tagger/model/final-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 1497/1497 [00:32<00:00, 62.47it/s]"
     ]
    }
   ],
   "source": [
    "#Extract conclusion target and stances for dev_sample_all\n",
    "dev_df = pd.read_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/valid_conclusion_all_sample.pkl')\n",
    "dev_df = dev_df[dev_df.title.str.len() > 0]\n",
    "dev_df = extract_targets_and_stances(dev_df)\n",
    "dev_df.to_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/valid_conclusion_all_sample_with_targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-08 13:01:03,790 loading file ../../../data-ceph/arguana/arg-generation/claim-target-tagger/model/final-model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853966f7e3144aa28927e9df20295767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 1899/1899 [00:42<00:00, 59.82it/s]"
     ]
    }
   ],
   "source": [
    "#Extract conclusion target and stances for test_sample_all\n",
    "dev_df = pd.read_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/test_concusion_all_sample.pkl')\n",
    "dev_df = dev_df[dev_df.title.str.len() > 0]\n",
    "dev_df = extract_targets_and_stances(dev_df)\n",
    "dev_df.to_pickle('../../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/test_conclusion_all_sample_with_targets.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16043b4afea6df4cc9c8277bea4f74cd7012ce4985455d3fd8e496ab2325b686"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
