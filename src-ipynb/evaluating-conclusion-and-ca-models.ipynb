{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "import re\n",
    "import random\n",
    "from argparse import Namespace\n",
    "\n",
    "sys.path.append('../src-py/')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "from utils import *\n",
    "from project_debater_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-09 10:56:03,792 loading file ../../data-ceph/arguana/arg-generation/claim-target-tagger/model/final-model.pt\n",
      "2022-08-09 10:56:29,236 SequenceTagger predicts: Dictionary with 4 tags: <unk>, B-CT, I-CT, O\n"
     ]
    }
   ],
   "source": [
    "from ca_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## General:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceph_dir = '/home/sile2804/data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation'\n",
    "local_home_dir = '../data'\n",
    "\n",
    "data_unique_path = '/reddit_data/conclusion_and_ca_generation/test_conclusion_all_preprocessed.pkl'\n",
    "#data_unique_path = '/reddit_data/conclusion_and_ca_generation/sample_test_conclusion_all_preprocessed.pkl'\n",
    "data_path = '/reddit_data/conclusion_and_ca_generation/test_conclusion_all.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ds_attacks(ds, model, tokenizer, premises_clm, conclusion_clm, gen_kwargs, skip_special_tokens=True, batch_size=5):\n",
    "    \n",
    "    ds = ds.map(lambda x :preprocess_function(x, tokenizer, premises_clm, 'counter', conclusion_clm=conclusion_clm), batched=True)\n",
    "    ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    dataloader = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "    attacks = generate_counters(model, tokenizer, dataloader, gen_kwargs, skip_special_tokens=skip_special_tokens)\n",
    "    \n",
    "    return attacks\n",
    "\n",
    "def create_predictions_df(reddit_sample_valid_ds, gen_kwargs, premises_clm='masked_premises'):\n",
    "    \n",
    "    #joint_attacks       = generate_ds_attacks(reddit_sample_valid_ds, join_model, join_model_tokenizer, premises_clm, None, gen_kwargs)\n",
    "    #noconc_attacks      = generate_ds_attacks(reddit_sample_valid_ds, no_conclusion_model, no_conclusion_tokenizer, premises_clm, None, gen_kwargs)\n",
    "    \n",
    "    known_conc_attacks  = generate_ds_attacks(reddit_sample_valid_ds, known_conclusion_model, known_conclusion_tokenizer, premises_clm, 'title', gen_kwargs)\n",
    "    bart_conc_attacks   = generate_ds_attacks(reddit_sample_valid_ds, known_conclusion_model, known_conclusion_tokenizer, premises_clm, 'bart_conclusion', gen_kwargs)\n",
    "    masked_conc_attacks = generate_ds_attacks(reddit_sample_valid_ds, known_conclusion_model, known_conclusion_tokenizer, premises_clm, None, gen_kwargs)\n",
    "    \n",
    "    #update max_gen_length to account to the generated conclusion\n",
    "    gen_kwargs['max_length'] = gen_kwargs['max_length'] + 50\n",
    "    joint_conc_baseline_attacks  = generate_ds_attacks(reddit_sample_valid_ds, pred_conclusion_model, pred_conclusion_tokenizer, premises_clm, None, gen_kwargs, skip_special_tokens=False)\n",
    "    \n",
    "    #gen_kwargs['max_length'] = gen_kwargs['max_length'] + 50\n",
    "    #planning_attacks         = generate_ds_attacks(reddit_sample_valid_ds, planning_model, planning_model_tokenizer, premises_clm, None, gen_kwargs, skip_special_tokens=False)\n",
    "\n",
    "    reddit_pred_df = pd.DataFrame(list(zip(\n",
    "                                           reddit_sample_valid_ds['post_id'],\n",
    "                                           reddit_sample_valid_ds['title'], \n",
    "                                           reddit_sample_valid_ds['conclusion_targets'],\n",
    "                                           reddit_sample_valid_ds['conclusion_stance'],\n",
    "                                           reddit_sample_valid_ds['bart_conclusion'], \n",
    "                                           reddit_sample_valid_ds[premises_clm],\n",
    "                                           reddit_sample_valid_ds['counter'], \n",
    "                                           known_conc_attacks, masked_conc_attacks, \n",
    "                                           bart_conc_attacks, joint_conc_baseline_attacks)), \n",
    "                    columns=['post_id', 'conclusion', 'conclusion_target', 'conclusion_stance', 'bart_conclusion', \n",
    "                             'premises', 'gt_attack', 'known_conc_attacks', 'masked_conc_attacks', \n",
    "                             'bart_conc_attacks',  'joint_conc_baseline_attacks'])\n",
    "\n",
    "    reddit_pred_df['argument'] = reddit_pred_df.apply(lambda row: row['conclusion'] + ' : ' + ' '.join(row['premises']), axis=1)\n",
    "    reddit_pred_df['premises'] = reddit_pred_df['premises'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    #process the jointly generated conclusion and counter\n",
    "    reddit_pred_df['joint_conc_baseline'] = reddit_pred_df['joint_conc_baseline_attacks'].apply (lambda x: x.split('<counter>')[0])\n",
    "    reddit_pred_df['joint_conc_baseline_attacks'] = reddit_pred_df['joint_conc_baseline_attacks'].apply (lambda x: x.split('<counter>')[1] if '<counter>' in x else x)\n",
    "    reddit_pred_df['joint_conc_baseline'] = reddit_pred_df['joint_conc_baseline'].apply (lambda x: re.sub('<s>|</s>|<conclusion>|<counter>|<pad>', '', x).strip())\n",
    "    reddit_pred_df['joint_conc_baseline_attacks'] = reddit_pred_df['joint_conc_baseline_attacks'].apply (lambda x: re.sub('<s>|</s>|<conclusion>|<counter>|<pad>', '', x).strip())\n",
    "\n",
    "    #similar to the jointly generated conclusion and counter\n",
    "    #reddit_pred_df['planning_counter_conclusion']  = reddit_pred_df['planning_attacks'].apply (lambda x: x.split('<claim>')[0])\n",
    "    #reddit_pred_df['planning_counter_attack'] = reddit_pred_df['planning_attacks'].apply (lambda x: x.split('<claim>')[1] if '<claim>' in x else x)\n",
    "    #reddit_pred_df['planning_counter_conclusion']  = reddit_pred_df['planning_counter_conclusion'].apply (lambda x: re.sub('<s>|</s>|<conclusion>|<counter>|<pad>', '', x).strip())\n",
    "    #reddit_pred_df['planning_counter_attack'] = reddit_pred_df['planning_counter_attack'].apply (lambda x: re.sub('<s>|</s>|<conclusion>|<counter>|<pad>', '', x).strip())\n",
    "\n",
    "    \n",
    "    return reddit_pred_df\n",
    "\n",
    "    \n",
    "def get_evaluation_results(reddit_pred_df, df_path, pred_clms, batched=False, detailed=False):\n",
    "    \n",
    "    #collect references\n",
    "    df = pd.read_pickle(df_path)\n",
    "    #Remove the first character from counter if it is a special character...\n",
    "    df['counter'] = df['counter'].apply(lambda counter_sents: [counter_sent for counter_sent in counter_sents if not counter_sent.startswith('!') and not counter_sent.startswith('.') and not counter_sent.startswith('?')])\n",
    "    arg_counters = df.groupby('post_id').agg({\n",
    "        'counter': lambda x: [' '.join(c) for c in x]\n",
    "    }).reset_index()\n",
    "\n",
    "    arg_counters = pd.Series(arg_counters.counter.values, index=arg_counters.post_id).to_dict()\n",
    "\n",
    "    reddit_pred_df['all_counters'] = reddit_pred_df['post_id'].apply(lambda x: arg_counters[x])\n",
    "    reddit_pred_df['all_counters'] = reddit_pred_df.all_counters.apply(lambda claims: [c for c in claims[:50] if len(c) > 0])\n",
    "    reddit_pred_df = reddit_pred_df[reddit_pred_df.all_counters.map(len) > 0]\n",
    "\n",
    "\n",
    "    #Test BLEU and BERT scores\n",
    "    eval_results = {}\n",
    "    for clm, clm_name in pred_clms.items():\n",
    "        eval_results[clm] = evaluate_gen_attacks(reddit_pred_df[clm_name].tolist(), reddit_pred_df['all_counters'].tolist(), detailed=detailed, batched=batched)\n",
    "        if detailed:\n",
    "            reddit_pred_df['{}_bleu'.format(clm_name)] = eval_results[clm]['bleu_scores']\n",
    "            reddit_pred_df['{}_bert'.format(clm_name)] = eval_results[clm]['bert-fscores']\n",
    "            \n",
    "    #Test stance correctness\n",
    "    #filtered_reddit_pred_df = reddit_pred_df[pd.notna(reddit_pred_df.conclusion_target)]\n",
    "    #print('Testing stance on only {} posts'.format(len(filtered_reddit_pred_df)))\n",
    "    \n",
    "    for clm, clm_name in pred_clms.items():\n",
    "        reddit_pred_df['{}_stances'.format(clm_name)] = get_stances(reddit_pred_df.conclusion_target.tolist(), \n",
    "                                                                        reddit_pred_df[clm_name].tolist())\n",
    "        eval_results[clm]['stance_score'] = round(np.mean([abs(x[0] - x[1]) for x in zip(reddit_pred_df['{}_stances'.format(clm_name)].tolist(), \n",
    "                                                                                         reddit_pred_df.conclusion_stance.tolist())]), 3)\n",
    "    \n",
    "    #compute stance scores using our trained stance classifier\n",
    "    for clm, clm_name in pred_clms.items():\n",
    "        _, labels, scores = get_stance_scores(reddit_pred_df.conclusion.tolist(), reddit_pred_df[clm_name].tolist())\n",
    "        scores = [x[1] if x[0] == 1 else -1 * x[1] for x in zip(labels, scores)] #postivie if opposing stance otherwise negative\n",
    "        eval_results[clm]['our_stance_score'] = (np.mean(scores), labels, scores)\n",
    "        if detailed:\n",
    "            reddit_pred_df['{}_our_stance_score'.format(clm_name)] = scores\n",
    "    \n",
    "    #compute argumentative quality\n",
    "    for clm, clm_name in pred_clms.items():\n",
    "        eval_results[clm]['quality_score'] = get_arg_quality(reddit_pred_df[clm_name].tolist())\n",
    "        \n",
    "        if detailed:\n",
    "            reddit_pred_df['{}_quality_score'.format(clm_name)] = eval_results[clm]['quality_score'][1]\n",
    "        \n",
    "    return eval_results, reddit_pred_df\n",
    "\n",
    "\n",
    "def print_results(pred_df_scores, stances_df, pred_clms, print_sig=False, sig_lvl=0.1):\n",
    "    for key in pred_clms:\n",
    "        for key2 in ['bleu', 'bert-fscore']:\n",
    "            pred_df_scores[key][key2] = round(pred_df_scores[key][key2], 3)\n",
    "    \n",
    "    results = []\n",
    "    for clm, _ in pred_clms.items():\n",
    "        clm_res = pred_df_scores[clm]\n",
    "        results.append([clm] + [clm_res['bleu'], clm_res['bert-fscore'], clm_res['stance_score'], clm_res['quality_score'][0], clm_res['our_stance_score'][0]])\n",
    "\n",
    "    res_table = tabulate(results, headers=['bleu', 'bert-f1score', 'stance-score (diff)', 'arg-quality', 'stance-score-2'])\n",
    "    \n",
    "    print(res_table)\n",
    "    \n",
    "    \n",
    "    if print_sig:\n",
    "        \n",
    "        #masked_conc_stance_scores= [abs(x[0] - x[1]) for x in zip(stances_df.masked_conc_attacks_stances.tolist(), stances_df.conclusion_stance.tolist())]\n",
    "        #bart_conc_stance_scores  = [abs(x[0] - x[1]) for x in zip(stances_df.bart_conc_attacks_stances.tolist(), stances_df.conclusion_stance.tolist())]\n",
    "        #pred_conc_stance_scores  = [abs(x[0] - x[1]) for x in zip(stances_df.joint_conc_baseline_attacks_stances.tolist(), stances_df.conclusion_stance.tolist())]\n",
    "        #multi_conc_pipeline_stance_scores = [abs(x[0] - x[1]) for x in zip(stances_df.multi_counter_pipeline_stances.tolist(), stances_df.conclusion_stance.tolist())]\n",
    "        #multi_conc_joint_stance_scores    = [abs(x[0] - x[1]) for x in zip(stances_df.multi_counter_joint_stances.tolist(), stances_df.conclusion_stance.tolist())]\n",
    "        #known_conc_stance_scores = [abs(x[0] - x[1]) for x in zip(stances_df.known_conc_attacks_stances.tolist(), stances_df.conclusion_stance.tolist())]\n",
    "        #shared_encoder_stance_scores = [abs(x[0] - x[1]) for x in zip(stances_df.shared_encoder_preds_stances.tolist(), stances_df.conclusion_stance.tolist())]\n",
    "        for clm, _ in pred_clms.items():\n",
    "            print('=======')\n",
    "            #Check significancy:\n",
    "            print('{} vs baseline (BLEU):'.format(clm), check_sig(pred_df_scores[clm]['bleu_scores'], pred_df_scores['Masked Conclusion']['bleu_scores'], alpha=sig_lvl))\n",
    "            print('{} vs baseline (BERT):'.format(clm), check_sig(pred_df_scores[clm]['bert-fscores'], pred_df_scores['Masked Conclusion']['bert-fscores'], alpha=sig_lvl))\n",
    "            #print('BART vs baseline (STANCE):', check_sig(bart_conc_stance_scores, masked_conc_stance_scores, alpha=sig_lvl))\n",
    "            print('{} vs baseline (STANCE-2):'.format(clm), check_sig(pred_df_scores[clm]['our_stance_score'][1], pred_df_scores['Masked Conclusion']['our_stance_score'][1], alpha=sig_lvl))\n",
    "        \n",
    "    return res_table\n",
    "\n",
    "\n",
    "def generate_manual_eval_df(df, clms=['masked_conc_attacks', 'multi_counter_joint', 'pred_counter_arguments_no_stance', 'bart_conc_attacks']):\n",
    "    data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        i = 0\n",
    "        random.shuffle(clms)\n",
    "        for clm in clms :\n",
    "            if i == 0:\n",
    "                data.append([row['post_id'], row['argument'], row[clm], clm])\n",
    "            else:\n",
    "                data.append(['', '', row[clm], clm])\n",
    "            i+=1\n",
    "    \n",
    "    res_df = pd.DataFrame(data, columns=['ID', 'Argument', 'Counter', 'Approach name'])\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Generated Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading Reddit models\n",
    "#no_conclusion_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').to(device)\n",
    "#no_conclusion_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "#planning_model = BartForConditionalGeneration.from_pretrained(local_home_dir + '/output/ca-final-models/planning-model/checkpoint-9000').to(device)\n",
    "#planning_model_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/planning-model/checkpoint-9000')\n",
    "#join_model_dws_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/mt-model-dynamic-weighting-scheme/checkpoint-9500')\n",
    "#join_model_dws  = BartModelV2.from_pretrained(local_home_dir + '/output/ca-final-models/mt-model-dynamic-weighting-scheme/checkpoint-9500', compute_dynamic_weights=True, conc_decoder=True).to(device)\n",
    "#join_model_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/mt-model-baseline-weighting-scheme/checkpoint-9500')\n",
    "#join_model  = BartModelV2.from_pretrained(local_home_dir + '/output/ca-final-models/mt-model-baseline-weighting-scheme/checkpoint-9500', compute_dynamic_weights=False, conc_decoder=True).to(device)\n",
    "\n",
    "known_conclusion_model = BartForConditionalGeneration.from_pretrained(local_home_dir + '/output/ca-final-models/known-conc-model/checkpoint-9500').to(device)\n",
    "known_conclusion_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/known-conc-model/checkpoint-9500')\n",
    "\n",
    "pred_conclusion_model = BartForConditionalGeneration.from_pretrained(local_home_dir + '/output/ca-final-models/pred-conc-model').to(device)\n",
    "pred_conclusion_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/pred-conc-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_pickle(ceph_dir + data_unique_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a dataset\n",
    "print('Testing on {} posts'.format(len(valid_df)))\n",
    "valid_ds = Dataset.from_pandas(valid_df)\n",
    "valid_ds = valid_ds.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate counters according to best parameters...\n",
    "gen_kwargs = {\n",
    "    \"do_sample\": False, \n",
    "    \"max_length\":100,\n",
    "    \"min_length\":50,\n",
    "    \"no_repeat_ngram_size\":3,\n",
    "    \"top_p\":0.95, \n",
    "    \"num_beams\":4\n",
    "}\n",
    "\n",
    "#generate predictions\n",
    "reddit_pred_df = create_predictions_df(valid_ds, gen_kwargs, premises_clm='post')\n",
    "#reddit_pred_df.to_pickle('../data/output/reddit_pred_test_no_sampling_4beam_df.pkl')\n",
    "reddit_pred_df.to_pickle('../data/output/test_all_reddit_pred_test_no_sampling_4beam_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate counters according to best parameters...\n",
    "gen_kwargs = {\n",
    "    \"do_sample\": True, \n",
    "    \"max_length\":100,\n",
    "    \"min_length\":50,\n",
    "    \"top_k\": 50,\n",
    "    \"no_repeat_ngram_size\":3,\n",
    "    \"top_p\":0.95, \n",
    "    \"num_beams\":4\n",
    "}\n",
    "\n",
    "#generate predictions\n",
    "reddit_pred_df = create_predictions_df(valid_ds, gen_kwargs, premises_clm='post')\n",
    "#reddit_pred_df.to_pickle('../data/output/reddit_pred_test_with_sampling_4beam_df.pkl')\n",
    "reddit_pred_df.to_pickle('../data/output/test_all_reddit_pred_test_with_sampling_4beam_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load prediction dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit_pred_df = pd.read_pickle('../data/output/reddit_pred_test_with_sampling_1beam_df.pkl')\n",
    "#reddit_pred_df = pd.read_pickle('../data/output/reddit_pred_test_with_sampling_4beam_df.pkl')\n",
    "#reddit_pred_df = pd.read_pickle('../data/output/reddit_pred_test_no_sampling_4beam_df.pkl')\n",
    "#reddit_pred_df = pd.read_pickle('../data/output/reddit_pred_test_with_sampling_4beam_df.pkl')\n",
    "reddit_pred_df = pd.read_pickle('../data/output/test_all_reddit_pred_test_with_sampling_4beam_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_counter_preds_via_joint_model = pd.read_pickle('../data/output/test_all_multi_conclusions_via_joint_model_withsampling_4beams.pkl')\n",
    "multi_counter_preds_via_pipeline_model = pd.read_pickle('../data/output/test_all_multi_conclusions_via_pipeline_model_withsample_4beam.pkl')\n",
    "\n",
    "two_decoders_preds = pd.read_pickle('../data/output/ca-final-models/mt-v4/results/all_test_preds_df.pkl')\n",
    "gpt_baseline = pd.read_pickle('../data/output/arg-undermining-baseline/test_preds.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "reddit_pred_df = reddit_pred_df.merge(multi_counter_preds_via_joint_model[['post_id', 'best_counter']], on=['post_id'])\n",
    "reddit_pred_df= reddit_pred_df.rename(columns={'best_counter': 'multi_counter_joint'})\n",
    "\n",
    "reddit_pred_df = reddit_pred_df.merge(multi_counter_preds_via_pipeline_model[['post_id', 'best_counter']], on=['post_id'])\n",
    "reddit_pred_df= reddit_pred_df.rename(columns={'best_counter': 'multi_counter_pipeline'})\n",
    "\n",
    "reddit_pred_df = reddit_pred_df.merge(two_decoders_preds, on=['post_id'])\n",
    "\n",
    "reddit_pred_df = reddit_pred_df.merge(gpt_baseline[['post_id', 'app_v2_attack_on_3_weak_premises']], on=['post_id'])\n",
    "reddit_pred_df= reddit_pred_df.rename(columns={'app_v2_attack_on_3_weak_premises': 'arg_undermining_pred'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_id', 'conclusion', 'conclusion_target', 'conclusion_stance',\n",
       "       'bart_conclusion', 'premises', 'gt_attack', 'known_conc_attacks',\n",
       "       'masked_conc_attacks', 'bart_conc_attacks',\n",
       "       'joint_conc_baseline_attacks', 'argument', 'joint_conc_baseline',\n",
       "       'multi_counter_joint', 'multi_counter_pipeline', 'title', 'post',\n",
       "       'counter', 'all_pred_counter_arguments_no_stance',\n",
       "       'all_pred_conclusions_no_stance', 'pred_counter_arguments_no_stance',\n",
       "       'pred_conclusions_no_stance', 'single_pred_counter_arguments_no_stance',\n",
       "       'arg_undermining_pred'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_pred_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clms = {  \n",
    "    # 'Masked Conclusion': 'masked_conc_attacks',\n",
    "    # 'BART Conclusion': 'bart_conc_attacks',\n",
    "    # 'Joint Prediction': 'joint_conc_baseline_attacks',\n",
    "    # 'Multi Conclusions (pipeline prediction)': 'multi_counter_pipeline',\n",
    "    # 'Multi Conclusions (joint prediction)': 'multi_counter_joint',\n",
    "    # 'Stance Based CAG (w/o stance)': 'single_pred_counter_arguments_no_stance',\n",
    "    # 'Stance Based CAG (M- w/o stance)': 'pred_counter_arguments_no_stance',\n",
    "    # 'Known Conclusion': 'known_conc_attacks',\n",
    "    'Arg-undermining baseline': 'arg_undermining_pred',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 8533/8533 [02:37<00:00, 54.08it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_results, eval_df = get_evaluation_results(reddit_pred_df, ceph_dir + data_path, pred_clms=pred_clms, batched=False, detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            bleu    bert-f1score    stance-score (diff)    arg-quality    stance-score-2\n",
      "------------------------  ------  --------------  ---------------------  -------------  ----------------\n",
      "Arg-undermining baseline   0.072           0.093                  0.805           0.67          0.664052\n",
      "=======\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Masked Conclusion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6191c227ed9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_clms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_sig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig_lvl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-16ef0430baa5>\u001b[0m in \u001b[0;36mprint_results\u001b[0;34m(pred_df_scores, stances_df, pred_clms, print_sig, sig_lvl)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m#Check significancy:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} vs baseline (BLEU):'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_sig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_df_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bleu_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_df_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Masked Conclusion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bleu_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msig_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} vs baseline (BERT):'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_sig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_df_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bert-fscores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_df_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Masked Conclusion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bert-fscores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msig_lvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;31m#print('BART vs baseline (STANCE):', check_sig(bart_conc_stance_scores, masked_conc_stance_scores, alpha=sig_lvl))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Masked Conclusion'"
     ]
    }
   ],
   "source": [
    "x = print_results(eval_results, eval_df, pred_clms, print_sig=True, sig_lvl=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = split_dataframe_per_conc_similarity(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['arg_len'] = eval_df.premises.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clm_name, clm in pred_clms.items():\n",
    "    eval_df['{}_opposing'.format(clm)] = eval_df.apply(lambda row: 1 if row['{}_stances'.format(clm)] * row['conclusion_stance'] < 0 else 0, axis=1)\n",
    "    print('{} score is: {}'.format(clm_name, eval_df['{}_opposing'.format(clm)].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df[['conclusion', 'conclusion_target', 'conclusion_stance', '{}_opposing'.format(clm), '{}_stances'.format(clm)]].head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df[(eval_df.max_sim_to_conclusion > 0.0) & (eval_df.arg_len < 300) & (eval_df.masked_conc_attacks_opposing < eval_df.multi_counter_joint_opposing)][['conclusion', 'argument', 'masked_conc_attacks', 'multi_counter_pipeline', 'multi_counter_joint', 'pred_counter_arguments_no_stance', 'known_conc_attacks']].sample(10).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_pickle('../data/output/automatic_evaluation_results_for_all_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generate a sample for manual evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_pred_df['arg_len'] = reddit_pred_df.argument.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_df = reddit_pred_df[reddit_pred_df.arg_len < 1000].sample(100)\n",
    "\n",
    "test_sample_df = generate_manual_eval_df(test_sample_df, clms=['masked_conc_attacks', 'multi_counter_pipeline', 'multi_counter_joint', 'pred_counter_arguments_no_stance'])\n",
    "test_sample_df.to_pickle('../data/manual_evaluation/eval_sample_all.pkl')\n",
    "test_sample_df[['ID', 'Argument', 'Counter']].to_csv('../data/manual_evaluation/eval_sample_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_df['arg_len'] = test_sample_df.Argument.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_df.arg_len.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Analyze automatic evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_pickle('../data/output/automatic_evaluation_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['arg_len'] = eval_df['argument'].apply(lambda x : len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = split_dataframe_per_conc_similarity(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clm_name, clm in pred_clms.items():\n",
    "    eval_df['{}_opposing'.format(clm)] = eval_df.apply(lambda row: 1 if row['{}_stances'.format(clm)] * row['conclusion_stance'] < 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.arg_len.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.max_sim_to_conclusion.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_effetiveness1(df, clm, measure, num_rows=10, num_clms=3):\n",
    "    df  = df.sort_values('arg_len')\n",
    "    chunk_size = int(len(df)/num_rows)\n",
    "    score_dist = np.zeros((num_rows, num_clms))\n",
    "    for i in range(0, num_rows) :\n",
    "        df_row = df[i*chunk_size: (i+1) * chunk_size].copy()\n",
    "        chunk_arg_len = round(df_row['arg_len'].mean(), 2)\n",
    "        print('Number of samples is {} with average argument length = {}'.format(len(df_row), chunk_arg_len))\n",
    "        #now split per similarity\n",
    "        df_row = df_row.sort_values('max_sim_to_conclusion')\n",
    "        chunk_size1 = int(len(df_row)/num_clms)\n",
    "        for j in range(0, num_clms):\n",
    "            df_chunk = df_row[j*chunk_size1: (j+1) * chunk_size1].copy()\n",
    "            chunk_sim = round(df_chunk['max_sim_to_conclusion'].mean(), 2)\n",
    "            print('Number of samples is {} with average similarity to conclusion = {}'.format(len(df_chunk), chunk_sim))\n",
    "            score_dist[i,j] = round(df_chunk['{}_{}'.format(clm, measure)].mean(), 2)\n",
    "    \n",
    "    return score_dist\n",
    "\n",
    "def analyze_effetiveness2(df, clm, measure, dimension='arg_len', num_buckets=5):\n",
    "    df  = df.sort_values(dimension)\n",
    "    chunk_size = int(len(df)/num_buckets)\n",
    "    score_dist = []\n",
    "    for i in range(0, num_buckets) :\n",
    "        df_chunk = df[i*chunk_size: (i+1) * chunk_size].copy()\n",
    "        chunk_dim = round(df_chunk[dimension].mean(), 2)\n",
    "        print('Number of samples is {} with average dimension length = {}'.format(len(df_chunk), chunk_dim))\n",
    "        value = round(df_chunk['{}_{}'.format(clm, measure)].mean(), 2)\n",
    "        score_dist.append((chunk_dim, value))\n",
    "    \n",
    "    return score_dist\n",
    "\n",
    "def analyze_effetiveness3(df, clm, measure, sim_thresholds=[0, 0.4, 0.7, 1.0], len_thresholds=[0, 300, 500, 1000]):\n",
    "    score_dist = np.zeros((len(len_thresholds)-1, len(sim_thresholds)-1))\n",
    "    for i in range(0, len(len_thresholds) - 1):\n",
    "        df_row = df[(df.arg_len >= len_thresholds[i]) & (df.arg_len < len_thresholds[i+1])]\n",
    "        chunk_arg_len = round(df_row['arg_len'].mean(), 2)\n",
    "        print('Number of samples is {} with average argument length = {}'.format(len(df_row), chunk_arg_len))\n",
    "        #now split per similarity\n",
    "        for j in range(0, len(sim_thresholds) - 1):\n",
    "            df_chunk = df_row[(df_row.max_sim_to_conclusion >= sim_thresholds[j]) & (df_row.max_sim_to_conclusion < sim_thresholds[j+1])]\n",
    "            chunk_sim = round(df_chunk['max_sim_to_conclusion'].mean(), 2)\n",
    "            print('Number of samples is {} with average similarity to conclusion = {}'.format(len(df_chunk), chunk_sim))\n",
    "            score_dist[i,j] = round(df_chunk['{}_{}'.format(clm, measure)].mean(), 3)\n",
    "    \n",
    "    return score_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_clms = {  \n",
    "#     'BART Conclusion': 'bart_conc_attacks',\n",
    "#     'Joint Prediction': 'joint_conc_baseline_attacks',\n",
    "#     'Multi Conclusions (pipeline prediction)': 'multi_counter_pipeline',\n",
    "     'Multi Conclusions (joint prediction)': 'multi_counter_joint',\n",
    "#     'Stance Based CAG (w/o stance)': 'single_pred_counter_arguments_no_stance',\n",
    "    #'Stance Based CAG (M- w/o stance)': 'pred_counter_arguments_no_stance',\n",
    "    'Known Conclusion': 'known_conc_attacks',\n",
    "    'Masked Conclusion': 'masked_conc_attacks',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness2(eval_df, clm, 'our_stance_score', 'max_sim_to_conclusion', num_buckets=4)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    print(app, app_scores)\n",
    "    xs, ys = zip(*app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/conc_sim_to_stance_score_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness2(eval_df, clm, 'bert', 'max_sim_to_conclusion', num_buckets=4)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    print(app, app_scores)\n",
    "    xs, ys = zip(*app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/conc_sim_to_bert_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness2(eval_df, clm, 'our_stance_score', 'arg_len', num_buckets=4)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    print(app, app_scores)\n",
    "    xs, ys = zip(*app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/arg_len_to_stance_score_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness2(eval_df, clm, 'bert', 'arg_len', num_buckets=4)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    xs, ys = zip(*app_scores)\n",
    "    print(app, app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/arg_len_to_bert_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#score_dist = analyze_effetiveness2(eval_df, 'masked_conc_attacks', 'bleu', len_thresholds=[0, 300, 600, 900, 1000], sim_thresholds=[0, 1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Manual evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_results_df = pd.read_excel('../data/manual_evaluation/results/counter_eval_mo.xlsx')\n",
    "sr_results_df = pd.read_excel('../data/manual_evaluation/results/counter_eval_sr.xlsx')\n",
    "sm_results_df = pd.read_excel('../data/manual_evaluation/results/counter_eval_sm.xlsx')\n",
    "\n",
    "eval_df = pd.read_pickle('../data/manual_evaluation/eval_sample_all.pkl')\n",
    "\n",
    "all_ann_df = mo_results_df\n",
    "\n",
    "all_ann_df['Rank2']    = sr_results_df['Rank'].tolist()\n",
    "all_ann_df['Comment2'] = sr_results_df['Comment'].tolist()\n",
    "\n",
    "all_ann_df['Rank3']    = sm_results_df['Rank'].tolist()\n",
    "all_ann_df['Comment3'] = sm_results_df['Comment'].tolist()\n",
    "\n",
    "all_ann_df['app_name'] = eval_df['Approach name'].tolist()\n",
    "all_ann_df['avg_rank'] = all_ann_df.apply(lambda row: (row['Rank'] + row['Rank2'] + row['Rank3'])/3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['num_sents'] = eval_df['Argument'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "eval_df['num_count_sents'] = eval_df['Counter'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "\n",
    "arg_len_dict = pd.Series(eval_df.num_sents.values, index=eval_df.ID).to_dict()\n",
    "all_ann_df['argument_len'] = all_ann_df.ID.apply(lambda x: arg_len_dict[x] if x is not np.nan else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This directly change the values in the dataframe\n",
    "args = all_ann_df.Argument.values\n",
    "arg_lens = all_ann_df.argument_len.values\n",
    "for i in range(0, len(args), 4):\n",
    "    for j in range(0, 4):\n",
    "        args[i+j] = args[i]\n",
    "        arg_lens[i+j] = arg_lens[i]\n",
    "        \n",
    "all_ann_df['conclusion'] = all_ann_df.Argument.apply(lambda x: x.split(':')[0])\n",
    "all_ann_df['Argument'] = all_ann_df.Argument.apply(lambda x: ' '.join(x.split(':')[1:]))\n",
    "\n",
    "all_ann_df = split_dataframe_per_conc_similarity(all_ann_df, conc_clm='conclusion', premises_clm='Argument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(df, clm1, clm2):\n",
    "    ranks = []\n",
    "    for chunk in chunks(df, 4):\n",
    "        ranks.append((chunk[clm1].tolist(), chunk[clm2].tolist()))\n",
    "    \n",
    "    return ranks\n",
    "\n",
    "def compute_kendalls_tau(ranks):\n",
    "    conc = 0\n",
    "    disc = 0\n",
    "    for rank in ranks:\n",
    "        for i in range(0, 4):\n",
    "            for j in range(i+1, 4):\n",
    "                #print(rank)\n",
    "                #print(i,j)\n",
    "                #print(rank[0][i], rank[0][j])\n",
    "                #print(rank[1][i], rank[1][j])\n",
    "                if (rank[0][i] > rank[0][j] and rank[1][i] > rank[1][j]) or (rank[0][i] < rank[0][j] and rank[1][i] < rank[1][j]):\n",
    "                    conc+=1\n",
    "                #    print('conc+')\n",
    "                else:\n",
    "                    disc+=1\n",
    "                #    print('disc+')\n",
    "                \n",
    "    return (conc - disc) / (6 * len(ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = get_ranks(all_ann_df, 'Rank', 'Rank2')\n",
    "print('Rank vs Rank1: ', compute_kendalls_tau(ranks))\n",
    "\n",
    "ranks = get_ranks(all_ann_df, 'Rank', 'Rank3')\n",
    "print('Rank vs Rank3: ', compute_kendalls_tau(ranks))\n",
    "\n",
    "ranks = get_ranks(all_ann_df, 'Rank2', 'Rank3')\n",
    "print('Rank2 vs Rank3: ', compute_kendalls_tau(ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_df[['app_name', 'Rank']].groupby('app_name').agg({'Rank': lambda x: np.mean(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_df[['app_name', 'Rank2']].groupby('app_name').agg({'Rank2': lambda x: np.mean(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_df[['app_name', 'Rank3']].groupby('app_name').agg({'Rank3': lambda x: np.mean(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_df[['app_name', 'avg_rank']].groupby('app_name').agg({'avg_rank': lambda x: np.mean(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_majority_class(ranks):\n",
    "    c = Counter(ranks)\n",
    "    value, count = c.most_common()[0]\n",
    "    if count > 1:\n",
    "        return value\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "all_ann_df['majority'] = all_ann_df.apply(lambda row: compute_majority_class([row['Rank'], row['Rank2'], row['Rank3']]) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Majority perc', len(all_ann_df[all_ann_df['majority'] != 0])/400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_df[all_ann_df.majority != 0][['app_name', 'majority']].groupby('app_name').agg({'majority': lambda x: np.mean(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_df[all_ann_df.majority != 0][['app_name', 'majority']].groupby('app_name').agg({'majority': lambda x: np.median(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_effetiveness(all_ann_df, approach, measure, dimension='arg_len', num_buckets=5):\n",
    "    df  = all_ann_df[all_ann_df.app_name == approach]\n",
    "    df  = df.sort_values(dimension)\n",
    "    chunk_size = int(len(df)/num_buckets)\n",
    "    score_dist = []\n",
    "    for i in range(0, num_buckets) :\n",
    "        df_chunk = df[i*chunk_size: (i+1) * chunk_size].copy()\n",
    "        chunk_dim = round(df_chunk[dimension].mean(), 2)\n",
    "        #print('Number of samples is {} with average dimension length = {}'.format(len(df_chunk), chunk_dim))\n",
    "        all_ranks = df_chunk['Rank'].tolist() + df_chunk['Rank2'].tolist() + df_chunk['Rank3'].tolist()\n",
    "        #all_ranks = {x: all_ranks.count(x) for x in [1,2,3,4]}.items()\n",
    "        value = round(np.array(all_ranks).mean(), 2)\n",
    "        score_dist.append((chunk_dim, value))\n",
    "    \n",
    "    return score_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clms = {  \n",
    "     'Multi Conclusions (pipeline prediction)': 'multi_counter_pipeline',\n",
    "     'Multi Conclusions (joint prediction)': 'multi_counter_joint',\n",
    "    'Stance Based CAG (M- w/o stance)': 'pred_counter_arguments_no_stance',\n",
    "    'Masked Conclusion': 'masked_conc_attacks',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness(all_ann_df, clm, 'majority', dimension='max_sim_to_conclusion', num_buckets=5)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    print(app, app_scores)\n",
    "    xs, ys = zip(*app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/arg_len_to_manual_score_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness(all_ann_df, clm, 'majority', dimension='argument_len', num_buckets=2)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    print(app, app_scores)\n",
    "    xs, ys = zip(*app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/arg_len_to_manual_score_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2\n",
    "all_ann_df_with_maj = all_ann_df #all_ann_df[(all_ann_df.majority !=0)]\n",
    "print(len(all_ann_df_with_maj))\n",
    "for i, item, in enumerate(pred_clms.items()):\n",
    "    app = item[0]\n",
    "    #all_ranks = [(x , len(all_ann_df[(all_ann_df.app_name == item[1]) & (all_ann_df.avg_rank >= x) & (all_ann_df.avg_rank < x+1)])) for x in [1,2,3,4]]\n",
    "    #all_ranks = list(all_ann_df_with_maj[all_ann_df_with_maj.app_name == item[1]]['majority'].value_counts().to_dict().items())\n",
    "    #take all users' ranks into account\n",
    "    all_ranks = all_ann_df[all_ann_df.app_name == item[1]]['Rank'].tolist() + all_ann_df[all_ann_df.app_name == item[1]]['Rank2'].tolist() + all_ann_df[all_ann_df.app_name == item[1]]['Rank3'].tolist()\n",
    "    all_ranks = {x: all_ranks.count(x) for x in [1,2,3,4]}.items()\n",
    "    \n",
    "    app_scores = all_ranks # sorted(all_ranks, key=lambda x: -x[0])\n",
    "    \n",
    "    print(app, app_scores)\n",
    "    xs, ys = zip(*app_scores)\n",
    "    xs = [round(i) for i in xs]\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/manual_ranking_scores.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_ann_df[(all_ann_df.argument_len < 400)][['ID', 'conclusion', 'Argument', 'Counter', \"app_name\", \"majority\"]].head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A look at the comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import wordcloud\n",
    "import nltk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_comments = [str(x) for x in all_ann_df[all_ann_df.Rank == 1]['Comment'].tolist() + all_ann_df[all_ann_df.Rank2 == 1]['Comment2'].tolist() + all_ann_df[all_ann_df.Rank3 == 1]['Comment3'].tolist()]\n",
    "bad_comments  = [str(x) for x in all_ann_df[all_ann_df.Rank == 4]['Comment'].tolist() + all_ann_df[all_ann_df.Rank2 == 4]['Comment2'].tolist() + all_ann_df[all_ann_df.Rank3 == 4]['Comment3'].tolist()]\n",
    "\n",
    "our_good_comments = [str(x) for x in all_ann_df[(all_ann_df.Rank == 1) & (all_ann_df.app_name=='multi_counter_joint')]['Comment'].tolist() + all_ann_df[(all_ann_df.Rank2 == 1) & (all_ann_df.app_name=='multi_counter_joint')]['Comment2'].tolist() + all_ann_df[(all_ann_df.Rank3 == 1) & (all_ann_df.app_name=='multi_counter_joint')]['Comment3'].tolist()]\n",
    "our_bad_comments  = [str(x) for x in all_ann_df[(all_ann_df.Rank == 4) & (all_ann_df.app_name=='multi_counter_joint')]['Comment'].tolist() + all_ann_df[(all_ann_df.Rank2 == 4) & (all_ann_df.app_name=='multi_counter_joint')]['Comment2'].tolist() + all_ann_df[(all_ann_df.Rank3 == 4) & (all_ann_df.app_name=='multi_counter_joint')]['Comment3'].tolist()]\n",
    "\n",
    "baseline_good_comments = [str(x) for x in all_ann_df[(all_ann_df.Rank == 1) & (all_ann_df.app_name=='masked_conc_attacks')]['Comment'].tolist() + all_ann_df[(all_ann_df.Rank2 == 1) & (all_ann_df.app_name=='masked_conc_attacks')]['Comment2'].tolist() + all_ann_df[(all_ann_df.Rank3 == 1) & (all_ann_df.app_name=='masked_conc_attacks')]['Comment3'].tolist()]\n",
    "baseline_bad_comments  = [str(x) for x in all_ann_df[(all_ann_df.Rank == 4) & (all_ann_df.app_name=='masked_conc_attacks')]['Comment'].tolist() + all_ann_df[(all_ann_df.Rank2 == 4) & (all_ann_df.app_name=='masked_conc_attacks')]['Comment2'].tolist() + all_ann_df[(all_ann_df.Rank3 == 4) & (all_ann_df.app_name=='masked_conc_attacks')]['Comment3'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Our good comments:\n",
    "- ('argumentative language', 14) + ('is argumentative', 10)\n",
    "- ('is coherent', 11) + ('coherent', 19)\n",
    "- ('new perspective', 5) +  ('perspective', 8),\n",
    "- ('it clearly states', 3)\n",
    "- ('directly refutes', 3)\n",
    "- opposes it on multiple points 2\n",
    "- \n",
    "##### Our bad comments:\n",
    "- ('only slightly addresses', 7),\n",
    "- ('very vauge', 6),\n",
    "- ('essentially supports', 3),\n",
    "- ('unrelated', 3),\n",
    "- ('incorrect', 3),\n",
    "- ('not engage', 3),\n",
    "\n",
    "##### Baseline bad comments:\n",
    "- (\"doesn't oppose\", 6),\n",
    "- ('lacks coherency', 5),\n",
    "- ('repetitive', 4),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ngrams(all_comments):\n",
    "    comments_ngrams = [list(ngrams(c.lower().split(), 2)) + list(ngrams(c.lower().split(), 3)) + [[w] for w in c.lower().split()] for c in all_comments]\n",
    "    comments_ngrams = [' '.join(x) for l in comments_ngrams for x in l]\n",
    "    c = Counter(comments_ngrams)\n",
    "    return c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_ngrams(our_good_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_ngrams(our_bad_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_ngrams(baseline_bad_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = wordcloud.WordCloud(stopwords=stopwords.words('english')).generate(' '.join(bad_comments))\n",
    "    \n",
    "plt.imshow(word_cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = wordcloud.WordCloud(stopwords=stopwords.words('english')).generate(' '.join(good_comments))\n",
    "    \n",
    "plt.imshow(word_cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_results_df = pd.read_excel('../data/manual_evaluation/results/conclusion_eval_fs.xlsx')\n",
    "ji_results_df = pd.read_excel('../data/manual_evaluation/results/conclusion_eval_ji.xlsx')\n",
    "\n",
    "conclusion_eval_df = pd.read_csv('../data/manual_evaluation/eval_conclusion_sample.csv')\n",
    "\n",
    "all_ann_df = ji_results_df\n",
    "all_ann_df['Valid2']    = fs_results_df['Valid'].tolist()\n",
    "all_ann_df['app_name'] = conclusion_eval_df['Approach name'].tolist()\n",
    "all_ann_df['avg_score'] = all_ann_df.apply(lambda row: (row['Valid'] + row['Valid2'])/2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_df[['app_name', 'avg_score']].groupby('app_name').agg({'avg_score': lambda x: np.mean(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(all_ann_df.Valid, all_ann_df.Valid2, weights='linear')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
