{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "import re\n",
    "import random\n",
    "from argparse import Namespace\n",
    "\n",
    "sys.path.append('../src-py/')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-10 16:22:29,919 loading file ../../data-ceph/arguana/arg-generation/claim-target-tagger/model/final-model.pt\n",
      "2022-08-10 16:22:57,845 SequenceTagger predicts: Dictionary with 4 tags: <unk>, B-CT, I-CT, O\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "from utils import *\n",
    "from ca_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## General:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_home_dir = '../sample-data'\n",
    "\n",
    "data_unique_path = '../sample-data/test_conclusion_all_preprocessed.pkl'\n",
    "data_path = '../sample-data/test_conclusion_all.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ds_attacks(ds, model, tokenizer, premises_clm, conclusion_clm, gen_kwargs, skip_special_tokens=True, batch_size=5):\n",
    "    \n",
    "    ds = ds.map(lambda x :preprocess_function(x, tokenizer, premises_clm, 'counter', conclusion_clm=conclusion_clm), batched=True)\n",
    "    ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    dataloader = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "    attacks = generate_counters(model, tokenizer, dataloader, gen_kwargs, skip_special_tokens=skip_special_tokens)\n",
    "    \n",
    "    return attacks\n",
    "\n",
    "def create_predictions_df(reddit_sample_valid_ds, gen_kwargs, premises_clm='masked_premises'):\n",
    "   \n",
    "    known_conc_attacks  = generate_ds_attacks(reddit_sample_valid_ds, known_conclusion_model, known_conclusion_tokenizer, premises_clm, 'title', gen_kwargs)\n",
    "    bart_conc_attacks   = generate_ds_attacks(reddit_sample_valid_ds, known_conclusion_model, known_conclusion_tokenizer, premises_clm, 'bart_conclusion', gen_kwargs)\n",
    "    masked_conc_attacks = generate_ds_attacks(reddit_sample_valid_ds, known_conclusion_model, known_conclusion_tokenizer, premises_clm, None, gen_kwargs)\n",
    "    \n",
    "    #update max_gen_length to account to the generated conclusion\n",
    "    gen_kwargs['max_length'] = gen_kwargs['max_length'] + 50\n",
    "    joint_conc_baseline_attacks  = generate_ds_attacks(reddit_sample_valid_ds, pred_conclusion_model, pred_conclusion_tokenizer, premises_clm, None, gen_kwargs, skip_special_tokens=False)\n",
    "\n",
    "    reddit_pred_df = pd.DataFrame(list(zip(\n",
    "                                           reddit_sample_valid_ds['post_id'],\n",
    "                                           reddit_sample_valid_ds['title'], \n",
    "                                           reddit_sample_valid_ds['conclusion_targets'],\n",
    "                                           reddit_sample_valid_ds['conclusion_stance'],\n",
    "                                           reddit_sample_valid_ds['bart_conclusion'], \n",
    "                                           reddit_sample_valid_ds[premises_clm],\n",
    "                                           reddit_sample_valid_ds['counter'], \n",
    "                                           known_conc_attacks, masked_conc_attacks, \n",
    "                                           bart_conc_attacks, joint_conc_baseline_attacks)), \n",
    "                    columns=['post_id', 'conclusion', 'conclusion_target', 'conclusion_stance', 'bart_conclusion', \n",
    "                             'premises', 'gt_attack', 'known_conc_attacks', 'masked_conc_attacks', \n",
    "                             'bart_conc_attacks',  'joint_conc_baseline_attacks'])\n",
    "\n",
    "    reddit_pred_df['argument'] = reddit_pred_df.apply(lambda row: row['conclusion'] + ' : ' + ' '.join(row['premises']), axis=1)\n",
    "    reddit_pred_df['premises'] = reddit_pred_df['premises'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    #process the jointly generated conclusion and counter\n",
    "    reddit_pred_df['joint_conc_baseline'] = reddit_pred_df['joint_conc_baseline_attacks'].apply (lambda x: x.split('<counter>')[0])\n",
    "    reddit_pred_df['joint_conc_baseline_attacks'] = reddit_pred_df['joint_conc_baseline_attacks'].apply (lambda x: x.split('<counter>')[1] if '<counter>' in x else x)\n",
    "    reddit_pred_df['joint_conc_baseline'] = reddit_pred_df['joint_conc_baseline'].apply (lambda x: re.sub('<s>|</s>|<conclusion>|<counter>|<pad>', '', x).strip())\n",
    "    reddit_pred_df['joint_conc_baseline_attacks'] = reddit_pred_df['joint_conc_baseline_attacks'].apply (lambda x: re.sub('<s>|</s>|<conclusion>|<counter>|<pad>', '', x).strip())\n",
    "\n",
    "    return reddit_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generated Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_conclusion_model = BartForConditionalGeneration.from_pretrained(local_home_dir + '/models/known-conc-model/checkpoint-9500').to(device)\n",
    "known_conclusion_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/models/known-conc-model/checkpoint-9500')\n",
    "\n",
    "pred_conclusion_model = BartForConditionalGeneration.from_pretrained(local_home_dir + '/models/pred-conc-model').to(device)\n",
    "pred_conclusion_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/models/pred-conc-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_pickle( data_unique_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 8533 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e8ab95e9a0484e9dc8492ad8e008fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a dataset\n",
    "print('Testing on {} posts'.format(len(valid_df)))\n",
    "valid_ds = Dataset.from_pandas(valid_df.sample(10))\n",
    "valid_ds = valid_ds.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate counters according to best parameters...\n",
    "gen_kwargs = {\n",
    "    \"do_sample\": True, \n",
    "    \"max_length\":100,\n",
    "    \"min_length\":50,\n",
    "    \"top_k\": 50,\n",
    "    \"no_repeat_ngram_size\":3,\n",
    "    \"top_p\":0.95, \n",
    "    \"num_beams\":4\n",
    "}\n",
    "\n",
    "#generate predictions\n",
    "reddit_pred_df = create_predictions_df(valid_ds, gen_kwargs, premises_clm='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_pred_df.to_pickle('../data-sample/output/test_all_reddit_pred_test_with_sampling_4beam_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
