{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from scipy import stats\n",
    "import re\n",
    "import random\n",
    "from argparse import Namespace\n",
    "\n",
    "sys.path.append('../src-py/')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-10 16:22:29,919 loading file ../../data-ceph/arguana/arg-generation/claim-target-tagger/model/final-model.pt\n",
      "2022-08-10 16:22:57,845 SequenceTagger predicts: Dictionary with 4 tags: <unk>, B-CT, I-CT, O\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "from utils import *\n",
    "from ca_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## General:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceph_dir = '/home/sile2804/data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation'\n",
    "local_home_dir = '../data'\n",
    "\n",
    "data_unique_path = '/reddit_data/conclusion_and_ca_generation/test_conclusion_all_preprocessed.pkl'\n",
    "data_path = '/reddit_data/conclusion_and_ca_generation/test_conclusion_all.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ds_attacks(ds, model, tokenizer, premises_clm, conclusion_clm, gen_kwargs, skip_special_tokens=True, batch_size=5):\n",
    "    \n",
    "    ds = ds.map(lambda x :preprocess_function(x, tokenizer, premises_clm, 'counter', conclusion_clm=conclusion_clm), batched=True)\n",
    "    ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    dataloader = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "    attacks = generate_counters(model, tokenizer, dataloader, gen_kwargs, skip_special_tokens=skip_special_tokens)\n",
    "    \n",
    "    return attacks\n",
    "\n",
    "def create_predictions_df(reddit_sample_valid_ds, gen_kwargs, premises_clm='masked_premises'):\n",
    "   \n",
    "    known_conc_attacks  = generate_ds_attacks(reddit_sample_valid_ds, known_conclusion_model, known_conclusion_tokenizer, premises_clm, 'title', gen_kwargs)\n",
    "    bart_conc_attacks   = generate_ds_attacks(reddit_sample_valid_ds, known_conclusion_model, known_conclusion_tokenizer, premises_clm, 'bart_conclusion', gen_kwargs)\n",
    "    masked_conc_attacks = generate_ds_attacks(reddit_sample_valid_ds, known_conclusion_model, known_conclusion_tokenizer, premises_clm, None, gen_kwargs)\n",
    "    \n",
    "    #update max_gen_length to account to the generated conclusion\n",
    "    gen_kwargs['max_length'] = gen_kwargs['max_length'] + 50\n",
    "    joint_conc_baseline_attacks  = generate_ds_attacks(reddit_sample_valid_ds, pred_conclusion_model, pred_conclusion_tokenizer, premises_clm, None, gen_kwargs, skip_special_tokens=False)\n",
    "\n",
    "    reddit_pred_df = pd.DataFrame(list(zip(\n",
    "                                           reddit_sample_valid_ds['post_id'],\n",
    "                                           reddit_sample_valid_ds['title'], \n",
    "                                           reddit_sample_valid_ds['conclusion_targets'],\n",
    "                                           reddit_sample_valid_ds['conclusion_stance'],\n",
    "                                           reddit_sample_valid_ds['bart_conclusion'], \n",
    "                                           reddit_sample_valid_ds[premises_clm],\n",
    "                                           reddit_sample_valid_ds['counter'], \n",
    "                                           known_conc_attacks, masked_conc_attacks, \n",
    "                                           bart_conc_attacks, joint_conc_baseline_attacks)), \n",
    "                    columns=['post_id', 'conclusion', 'conclusion_target', 'conclusion_stance', 'bart_conclusion', \n",
    "                             'premises', 'gt_attack', 'known_conc_attacks', 'masked_conc_attacks', \n",
    "                             'bart_conc_attacks',  'joint_conc_baseline_attacks'])\n",
    "\n",
    "    reddit_pred_df['argument'] = reddit_pred_df.apply(lambda row: row['conclusion'] + ' : ' + ' '.join(row['premises']), axis=1)\n",
    "    reddit_pred_df['premises'] = reddit_pred_df['premises'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    #process the jointly generated conclusion and counter\n",
    "    reddit_pred_df['joint_conc_baseline'] = reddit_pred_df['joint_conc_baseline_attacks'].apply (lambda x: x.split('<counter>')[0])\n",
    "    reddit_pred_df['joint_conc_baseline_attacks'] = reddit_pred_df['joint_conc_baseline_attacks'].apply (lambda x: x.split('<counter>')[1] if '<counter>' in x else x)\n",
    "    reddit_pred_df['joint_conc_baseline'] = reddit_pred_df['joint_conc_baseline'].apply (lambda x: re.sub('<s>|</s>|<conclusion>|<counter>|<pad>', '', x).strip())\n",
    "    reddit_pred_df['joint_conc_baseline_attacks'] = reddit_pred_df['joint_conc_baseline_attacks'].apply (lambda x: re.sub('<s>|</s>|<conclusion>|<counter>|<pad>', '', x).strip())\n",
    "\n",
    "    return reddit_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generated Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_conclusion_model = BartForConditionalGeneration.from_pretrained(local_home_dir + '/output/ca-final-models/known-conc-model/checkpoint-9500').to(device)\n",
    "known_conclusion_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/known-conc-model/checkpoint-9500')\n",
    "\n",
    "pred_conclusion_model = BartForConditionalGeneration.from_pretrained(local_home_dir + '/output/ca-final-models/pred-conc-model').to(device)\n",
    "pred_conclusion_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/pred-conc-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_pickle(ceph_dir + data_unique_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 8533 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e8ab95e9a0484e9dc8492ad8e008fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a dataset\n",
    "print('Testing on {} posts'.format(len(valid_df)))\n",
    "valid_ds = Dataset.from_pandas(valid_df.sample(10))\n",
    "valid_ds = valid_ds.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate counters according to best parameters...\n",
    "gen_kwargs = {\n",
    "    \"do_sample\": True, \n",
    "    \"max_length\":100,\n",
    "    \"min_length\":50,\n",
    "    \"top_k\": 50,\n",
    "    \"no_repeat_ngram_size\":3,\n",
    "    \"top_p\":0.95, \n",
    "    \"num_beams\":4\n",
    "}\n",
    "\n",
    "#generate predictions\n",
    "reddit_pred_df = create_predictions_df(valid_ds, gen_kwargs, premises_clm='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_pred_df.to_pickle('../data/output/test_all_reddit_pred_test_with_sampling_4beam_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_pickle('../data/output/automatic_evaluation_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['arg_len'] = eval_df['argument'].apply(lambda x : len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = split_dataframe_per_conc_similarity(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clm_name, clm in pred_clms.items():\n",
    "    eval_df['{}_opposing'.format(clm)] = eval_df.apply(lambda row: 1 if row['{}_stances'.format(clm)] * row['conclusion_stance'] < 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.arg_len.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.max_sim_to_conclusion.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_effetiveness1(df, clm, measure, num_rows=10, num_clms=3):\n",
    "    df  = df.sort_values('arg_len')\n",
    "    chunk_size = int(len(df)/num_rows)\n",
    "    score_dist = np.zeros((num_rows, num_clms))\n",
    "    for i in range(0, num_rows) :\n",
    "        df_row = df[i*chunk_size: (i+1) * chunk_size].copy()\n",
    "        chunk_arg_len = round(df_row['arg_len'].mean(), 2)\n",
    "        print('Number of samples is {} with average argument length = {}'.format(len(df_row), chunk_arg_len))\n",
    "        #now split per similarity\n",
    "        df_row = df_row.sort_values('max_sim_to_conclusion')\n",
    "        chunk_size1 = int(len(df_row)/num_clms)\n",
    "        for j in range(0, num_clms):\n",
    "            df_chunk = df_row[j*chunk_size1: (j+1) * chunk_size1].copy()\n",
    "            chunk_sim = round(df_chunk['max_sim_to_conclusion'].mean(), 2)\n",
    "            print('Number of samples is {} with average similarity to conclusion = {}'.format(len(df_chunk), chunk_sim))\n",
    "            score_dist[i,j] = round(df_chunk['{}_{}'.format(clm, measure)].mean(), 2)\n",
    "    \n",
    "    return score_dist\n",
    "\n",
    "def analyze_effetiveness2(df, clm, measure, dimension='arg_len', num_buckets=5):\n",
    "    df  = df.sort_values(dimension)\n",
    "    chunk_size = int(len(df)/num_buckets)\n",
    "    score_dist = []\n",
    "    for i in range(0, num_buckets) :\n",
    "        df_chunk = df[i*chunk_size: (i+1) * chunk_size].copy()\n",
    "        chunk_dim = round(df_chunk[dimension].mean(), 2)\n",
    "        print('Number of samples is {} with average dimension length = {}'.format(len(df_chunk), chunk_dim))\n",
    "        value = round(df_chunk['{}_{}'.format(clm, measure)].mean(), 2)\n",
    "        score_dist.append((chunk_dim, value))\n",
    "    \n",
    "    return score_dist\n",
    "\n",
    "def analyze_effetiveness3(df, clm, measure, sim_thresholds=[0, 0.4, 0.7, 1.0], len_thresholds=[0, 300, 500, 1000]):\n",
    "    score_dist = np.zeros((len(len_thresholds)-1, len(sim_thresholds)-1))\n",
    "    for i in range(0, len(len_thresholds) - 1):\n",
    "        df_row = df[(df.arg_len >= len_thresholds[i]) & (df.arg_len < len_thresholds[i+1])]\n",
    "        chunk_arg_len = round(df_row['arg_len'].mean(), 2)\n",
    "        print('Number of samples is {} with average argument length = {}'.format(len(df_row), chunk_arg_len))\n",
    "        #now split per similarity\n",
    "        for j in range(0, len(sim_thresholds) - 1):\n",
    "            df_chunk = df_row[(df_row.max_sim_to_conclusion >= sim_thresholds[j]) & (df_row.max_sim_to_conclusion < sim_thresholds[j+1])]\n",
    "            chunk_sim = round(df_chunk['max_sim_to_conclusion'].mean(), 2)\n",
    "            print('Number of samples is {} with average similarity to conclusion = {}'.format(len(df_chunk), chunk_sim))\n",
    "            score_dist[i,j] = round(df_chunk['{}_{}'.format(clm, measure)].mean(), 3)\n",
    "    \n",
    "    return score_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_clms = {  \n",
    "#     'BART Conclusion': 'bart_conc_attacks',\n",
    "#     'Joint Prediction': 'joint_conc_baseline_attacks',\n",
    "#     'Multi Conclusions (pipeline prediction)': 'multi_counter_pipeline',\n",
    "     'Multi Conclusions (joint prediction)': 'multi_counter_joint',\n",
    "#     'Stance Based CAG (w/o stance)': 'single_pred_counter_arguments_no_stance',\n",
    "    #'Stance Based CAG (M- w/o stance)': 'pred_counter_arguments_no_stance',\n",
    "    'Known Conclusion': 'known_conc_attacks',\n",
    "    'Masked Conclusion': 'masked_conc_attacks',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness2(eval_df, clm, 'our_stance_score', 'max_sim_to_conclusion', num_buckets=4)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    print(app, app_scores)\n",
    "    xs, ys = zip(*app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/conc_sim_to_stance_score_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness2(eval_df, clm, 'bert', 'max_sim_to_conclusion', num_buckets=4)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    print(app, app_scores)\n",
    "    xs, ys = zip(*app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/conc_sim_to_bert_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness2(eval_df, clm, 'our_stance_score', 'arg_len', num_buckets=4)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    print(app, app_scores)\n",
    "    xs, ys = zip(*app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/arg_len_to_stance_score_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dist = {}\n",
    "for clm_name, clm in pred_clms.items():\n",
    "    score_dist[clm_name] = analyze_effetiveness2(eval_df, clm, 'bert', 'arg_len', num_buckets=4)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "width = 0.2 #/len(score_dist)\n",
    "for i, item, in enumerate(score_dist.items()):\n",
    "    app, app_scores = item[0], item[1]\n",
    "    xs, ys = zip(*app_scores)\n",
    "    print(app, app_scores)\n",
    "    plt.bar([x + (i * width) for x in range(0, len(xs))], ys, width=width , label=app,)\n",
    "\n",
    "plt.xticks(range(0, len(xs)), xs)\n",
    "plt.legend()\n",
    "plt.savefig('./figures/arg_len_to_bert_correlation.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#score_dist = analyze_effetiveness2(eval_df, 'masked_conc_attacks', 'bleu', len_thresholds=[0, 300, 600, 900, 1000], sim_thresholds=[0, 1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
