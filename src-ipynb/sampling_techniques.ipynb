{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from argparse import Namespace\n",
    "\n",
    "sys.path.append('../src-py/')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from utils import *\n",
    "from project_debater_api import *\n",
    "from mt_bart_v2 import *\n",
    "\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_train_conclusion_all.pkl\n",
      "sample_test_conclusion_all.pkl\n",
      "sample_test_conclusion_all_preprocessed.pkl\n",
      "sample_test_conclusion_all_preprocessed_multi_conclusions.pkl\n",
      "sample_valid_conclusion_all.pkl\n",
      "sample_valid_conclusion_all_preprocessed.pkl\n",
      "test_conclusion_all.pkl\n",
      "test_conclusion_comp_remove_75sem_perc.pkl\n",
      "train_conclusion_all.pkl\n",
      "train_conclusion_comp_remove_75sem_perc.pkl\n",
      "valid_conclusion_all.pkl\n",
      "valid_conclusion_comp_remove_75sem_perc.pkl\n"
     ]
    }
   ],
   "source": [
    "ls /home/sile2804/data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "ceph_dir = '/home/sile2804/data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation'\n",
    "local_home_dir = '../data'\n",
    "\n",
    "data_unique_path = '/reddit_data/conclusion_and_ca_generation/sample_valid_conclusion_all_preprocessed.pkl'\n",
    "data_path = '/reddit_data/conclusion_and_ca_generation/sample_valid_conclusion_all.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ds_attacks(ds, model, tokenizer, premises_clm, conclusion_clm, gen_kwargs, skip_special_tokens=True, batch_size=16):\n",
    "    ds = ds.map(lambda x :preprocess_function(x, tokenizer, premises_clm, 'counter', conclusion_clm=conclusion_clm), batched=True)\n",
    "    ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    dataloader = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n",
    "    attacks = generate_counters(model, tokenizer, dataloader, gen_kwargs, skip_special_tokens=skip_special_tokens)\n",
    "    \n",
    "    return attacks\n",
    "\n",
    "def create_predictions_df(model, tokenizer, reddit_sample_valid_ds, gen_kwargs, premises_clm='masked_premises', conclusion_clm=None):\n",
    "    \n",
    "    \n",
    "    known_conc_attacks  = generate_ds_attacks(reddit_sample_valid_ds, model, tokenizer, premises_clm, conclusion_clm, gen_kwargs)    \n",
    "    #update max_gen_length to account to the generated conclusion\n",
    "    \n",
    "    reddit_pred_df = pd.DataFrame(list(zip(\n",
    "                                           reddit_sample_valid_ds['post_id'],\n",
    "                                           reddit_sample_valid_ds['title'], \n",
    "                                           reddit_sample_valid_ds['conclusion_targets'],\n",
    "                                           reddit_sample_valid_ds['conclusion_stance'],\n",
    "                                           reddit_sample_valid_ds['bart_conclusion'], \n",
    "                                           reddit_sample_valid_ds[premises_clm],\n",
    "                                           reddit_sample_valid_ds['counter'], \n",
    "                                           known_conc_attacks)), \n",
    "                    columns=['post_id', 'conclusion', 'conclusion_target', 'conclusion_stance', 'bart_conclusion', 'premises', 'gt_attack', 'known_conc_attacks'])\n",
    "\n",
    "    reddit_pred_df['argument'] = reddit_pred_df.apply(lambda row: row['conclusion'] + ' : ' + ' '.join(row['premises']), axis=1)\n",
    "    reddit_pred_df['premises'] = reddit_pred_df['premises'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    return reddit_pred_df\n",
    "\n",
    "def get_evaluation_results(reddit_pred_df, df_path):\n",
    "    #collect references\n",
    "    df = pd.read_pickle(df_path)\n",
    "    arg_counters = df.groupby('post_id').agg({\n",
    "        'counter': lambda x: [' '.join(c) for c in x]\n",
    "    }).reset_index()\n",
    "\n",
    "    arg_counters = pd.Series(arg_counters.counter.values, index=arg_counters.post_id).to_dict()\n",
    "\n",
    "    reddit_pred_df['all_counters'] = reddit_pred_df['post_id'].apply(lambda x: arg_counters[x])\n",
    "    reddit_pred_df['all_counters'] = reddit_pred_df.all_counters.apply(lambda claims: [c for c in claims if c !=''])\n",
    "    reddit_pred_df = reddit_pred_df[reddit_pred_df.all_counters.map(len) > 0]\n",
    "\n",
    "    known_conc_eval  = evaluate_gen_attacks(reddit_pred_df['known_conc_attacks'].tolist(), reddit_pred_df['all_counters'].tolist(), detailed=False)\n",
    "  \n",
    "    #Test stance correctness\n",
    "    filtered_reddit_pred_df = reddit_pred_df[pd.notna(reddit_pred_df.conclusion_target)]\n",
    "    print('Testing stance on only {} posts'.format(len(filtered_reddit_pred_df)))\n",
    "    \n",
    "    #compute the stance of the generated counters towards the conclusion target\n",
    "    filtered_reddit_pred_df['known_conc_stances']  = get_stances(filtered_reddit_pred_df.conclusion_target.tolist(), filtered_reddit_pred_df.known_conc_attacks.tolist())\n",
    "    \n",
    "    #compute the distance between the conclusion stance and the attack stance: the bigger the distance the better...\n",
    "    known_conc_eval['stance_score']  = round(np.mean([abs(x[0] - x[1]) for x in zip(filtered_reddit_pred_df.known_conc_stances.tolist(), filtered_reddit_pred_df.conclusion_stance.tolist())]), 2)\n",
    "\n",
    "    return known_conc_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_conclusion_model = BartForConditionalGeneration.from_pretrained(local_home_dir + '/output/ca-final-models/known-conc-model/checkpoint-9500').to(device)\n",
    "known_conclusion_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/known-conc-model/checkpoint-9500')\n",
    "\n",
    "pred_conclusion_model = BartForConditionalGeneration.from_pretrained(local_home_dir + '/output/ca-final-models/pred-conc-model/checkpoint-9500').to(device)\n",
    "pred_conclusion_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/pred-conc-model/checkpoint-9500')\n",
    "\n",
    "join_model_tokenizer = BartTokenizer.from_pretrained(local_home_dir + '/output/ca-final-models/mt-model-baseline-weighting-scheme/checkpoint-9500')\n",
    "join_model  = BartModelV2.from_pretrained(local_home_dir + '/output/ca-final-models/mt-model-baseline-weighting-scheme/checkpoint-9500', compute_dynamic_weights=False, conc_decoder=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_pickle(ceph_dir + data_unique_path).sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 1000 posts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15e2084e91648999d77bda91b9d5792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a dataset\n",
    "print('Testing on {} posts'.format(len(valid_df)))\n",
    "valid_ds = Dataset.from_pandas(valid_df)\n",
    "valid_ds = valid_ds.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.to_pickle('../data/valid_sample_for_finetuning_sampling_techniques.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying the effect of Beam Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279c1b750e1145708519fd2e91f65624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:19<00:00, 51.97it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b1366870614198a2e7c017072607f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:18<00:00, 54.62it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afb38701394499db9c48ee18ca6cce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:18<00:00, 54.96it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217b632ef0794085b442a2147e28c88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:17<00:00, 55.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#Generate counters without sampling technique...\n",
    "gen_kwargs = {\n",
    "    \"do_sample\": True, \n",
    "    \"max_length\":100,\n",
    "    \"top_p\":0.95, #from fine-tuning the best top_p was 0.95\n",
    "    \"num_beams\":10\n",
    "}\n",
    "\n",
    "scores = []\n",
    "for num_beams in [1, 4, 8, 12]:\n",
    "    #generate predictions\n",
    "    gen_kwargs['num_beams'] = num_beams\n",
    "    preds_df = create_predictions_df(known_conclusion_model, known_conclusion_tokenizer, valid_ds, gen_kwargs, premises_clm='post', conclusion_clm='title')\n",
    "    preds_df_scores = get_evaluation_results(preds_df, ceph_dir + data_path)\n",
    "    scores.append([num_beams, preds_df_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  num_beams    bleu    bert-f1score    stance-score (diff)\n",
      "-----------  ------  --------------  ---------------------\n",
      "          1    0.01            0.03                   0.86\n",
      "          4    0.02            0.06                   0.82\n",
      "          8    0.02            0.07                   0.75\n",
      "         12    0.02            0.07                   0.75\n"
     ]
    }
   ],
   "source": [
    "res_table = tabulate([(x[0], round(x[1]['bleu'], 2), round(x[1]['bert-fscore'], 2), x[1]['stance_score']) for x in scores], headers=['num_beams', 'bleu', 'bert-f1score', 'stance-score (diff)'])\n",
    "    \n",
    "print(res_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb033a60206420fa06fbd7e6484de5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:19<00:00, 52.15it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bda6acc21843a08e6b3fde85b84fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:18<00:00, 54.02it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509d965bc1a84182a4363c5dc696e8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:18<00:00, 55.02it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb120bf53644094b633a239669ff572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:17<00:00, 55.57it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for num_beams in [1, 4, 8, 12]:\n",
    "    #generate predictions\n",
    "    gen_kwargs['num_beams'] = num_beams\n",
    "    preds_df = create_predictions_df(pred_conclusion_model, pred_conclusion_tokenizer, valid_ds, gen_kwargs, premises_clm='post', conclusion_clm=None)\n",
    "    preds_df_scores = get_evaluation_results(preds_df, ceph_dir + data_path)\n",
    "    scores.append([num_beams, preds_df_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  num_beams    bleu    bert-f1score    stance-score (diff)\n",
      "-----------  ------  --------------  ---------------------\n",
      "          1    0.01            0.01                   0.78\n",
      "          4    0.02            0.03                   0.73\n",
      "          8    0.02            0.03                   0.74\n",
      "         12    0.02            0.03                   0.69\n"
     ]
    }
   ],
   "source": [
    "res_table = tabulate([(x[0], round(x[1]['bleu'], 2), round(x[1]['bert-fscore'], 2), x[1]['stance_score']) for x in scores], headers=['num_beams', 'bleu', 'bert-f1score', 'stance-score (diff)'])\n",
    "    \n",
    "print(res_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfaee9679a647bf8a71b59857599ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:18<00:00, 52.87it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7725ed34cb344dcf881895dde6e64db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:18<00:00, 54.02it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9b1dffb0924880a38bd8f34104ec6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:18<00:00, 54.00it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3e5880e2df486f9e5b990864e40cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 999 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 999/999 [00:18<00:00, 54.90it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for num_beams in [1, 4, 8, 12]:\n",
    "    #generate predictions\n",
    "    gen_kwargs['num_beams'] = num_beams\n",
    "    preds_df = create_predictions_df(join_model, join_model_tokenizer, valid_ds, gen_kwargs, premises_clm='post', conclusion_clm=None)\n",
    "    preds_df_scores = get_evaluation_results(preds_df, ceph_dir + data_path)\n",
    "    scores.append([num_beams, preds_df_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  num_beams    bleu    bert-f1score    stance-score (diff)\n",
      "-----------  ------  --------------  ---------------------\n",
      "          1    0.01            0.03                   0.82\n",
      "          4    0.02            0.06                   0.84\n",
      "          8    0.02            0.06                   0.83\n",
      "         12    0.02            0.06                   0.83\n"
     ]
    }
   ],
   "source": [
    "res_table = tabulate([(x[0], round(x[1]['bleu'], 2), round(x[1]['bert-fscore'], 2), x[1]['stance_score']) for x in scores], headers=['num_beams', 'bleu', 'bert-f1score', 'stance-score (diff)'])\n",
    "    \n",
    "print(res_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying the effect of P:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate counters without sampling technique...\n",
    "gen_kwargs = {\n",
    "    \"do_sample\": True, \n",
    "    \"max_length\":100,\n",
    "    \"top_p\":0.95, \n",
    "    \"num_beams\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bea2c7ef10f4e1fa57d40422311c208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:09<00:00, 51.71it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8a428dec384d75a58424fe804c7892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:10<00:00, 47.30it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29363e26d7d847a9a48925556fd60950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:10<00:00, 48.23it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a828ed148f54441a94b771f843eec00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:11<00:00, 44.61it/s]\n"
     ]
    }
   ],
   "source": [
    "scores1 = []\n",
    "for p in [0.85, 0.90, 0.95, 1.0]:\n",
    "    #generate predictions\n",
    "    gen_kwargs['top_p'] = p\n",
    "    preds_df = create_predictions_df(known_conclusion_model, known_conclusion_tokenizer, valid_ds, gen_kwargs, premises_clm='post', conclusion_clm='title')\n",
    "    preds_df_scores = get_evaluation_results(preds_df, ceph_dir + data_path)\n",
    "    scores1.append([p, preds_df_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   p    bleu    bert-f1score    stance-score (diff)\n",
      "----  ------  --------------  ---------------------\n",
      "0.85    0.11            0.11                   0.71\n",
      "0.9     0.1             0.1                    0.67\n",
      "0.95    0.09            0.1                    0.66\n",
      "1       0.08            0.09                   0.68\n"
     ]
    }
   ],
   "source": [
    "res_table = tabulate([(x[0], round(x[1]['bleu'], 2), round(x[1]['bert-fscore'], 2), x[1]['stance_score']) for x in scores1], headers=['p', 'bleu', 'bert-f1score', 'stance-score (diff)'])\n",
    "    \n",
    "print(res_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a953cebc56404998dc7bd347132097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:09<00:00, 52.39it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c67f1dd477048fda94a323765dea8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:09<00:00, 52.67it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042fd26ea07a4692ace22b8ca34c2ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:09<00:00, 52.95it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b426728875c647ff929caa47d30cf0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:09<00:00, 50.37it/s]\n"
     ]
    }
   ],
   "source": [
    "scores2 = []\n",
    "for p in [0.85, 0.90, 0.95, 1.0]:\n",
    "    #generate predictions\n",
    "    gen_kwargs['top_p'] = p\n",
    "    preds_df = create_predictions_df(pred_conclusion_model, pred_conclusion_tokenizer, valid_ds, gen_kwargs, premises_clm='post', conclusion_clm=None)\n",
    "    preds_df_scores = get_evaluation_results(preds_df, ceph_dir + data_path)\n",
    "    scores2.append([p, preds_df_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   p    bleu    bert-f1score    stance-score (diff)\n",
      "----  ------  --------------  ---------------------\n",
      "0.85    0.09            0.08                   0.73\n",
      "0.9     0.08            0.08                   0.7\n",
      "0.95    0.08            0.08                   0.72\n",
      "1       0.07            0.07                   0.7\n"
     ]
    }
   ],
   "source": [
    "res_table = tabulate([(x[0], round(x[1]['bleu'], 2), round(x[1]['bert-fscore'], 2), x[1]['stance_score']) for x in scores2], headers=['p', 'bleu', 'bert-f1score', 'stance-score (diff)'])\n",
    "print(res_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07dfa986aed4ef6940ee18511797d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:09<00:00, 52.69it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d7c944979f4de9802f43109008aacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:11<00:00, 43.67it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3609a030b68f4345ae7ba46ef6eb5d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:11<00:00, 43.78it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c7b40ca3fe470da4635eebfbeb4d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stance on only 500 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProConClient: 100%|██████████| 500/500 [00:09<00:00, 51.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   p    bleu    bert-f1score    stance-score (diff)\n",
      "----  ------  --------------  ---------------------\n",
      "0.85    0.1             0.1                    0.73\n",
      "0.9     0.09            0.09                   0.67\n",
      "0.95    0.08            0.09                   0.74\n",
      "1       0.07            0.08                   0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores3 = []\n",
    "for p in [0.85, 0.90, 0.95, 1.0]:\n",
    "    #generate predictions\n",
    "    gen_kwargs['top_p'] = p\n",
    "    preds_df = create_predictions_df(join_model, join_model_tokenizer, valid_ds, gen_kwargs, premises_clm='post', conclusion_clm=None)\n",
    "    preds_df_scores = get_evaluation_results(preds_df, ceph_dir + data_path)\n",
    "    scores3.append([p, preds_df_scores])\n",
    "    \n",
    "\n",
    "res_table = tabulate([(x[0], round(x[1]['bleu'], 2), round(x[1]['bert-fscore'], 2), x[1]['stance_score']) for x in scores3], headers=['p', 'bleu', 'bert-f1score', 'stance-score (diff)'])\n",
    "print(res_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
