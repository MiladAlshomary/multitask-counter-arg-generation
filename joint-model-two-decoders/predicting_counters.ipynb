{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src-py/')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "import transformers\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import os\n",
    "from model import MultiTaskBart\n",
    "from model import OurModel\n",
    "from utils import parse_df\n",
    "import time\n",
    "import sys\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU?  True\n",
      "Device name: A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Using GPU? \", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "data_dir = '../../data-ceph/arguana/arg-generation/multi-taks-counter-argument-generation/reddit_data/conclusion_and_ca_generation/'\n",
    "teacher_model_path='../../multitask-counter-arg-generation/data/output/stance_classification/best_model/'\n",
    "\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import prompted_conclusion_utils as conc_utils\n",
    "from prompted_conclusion_utils import *\n",
    "\n",
    "from transformers.generation_logits_process import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teacher model\n",
    "stance_classifier_teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_path)\n",
    "stance_classifier_teacher_model     = AutoModelForSequenceClassification.from_pretrained(teacher_model_path)\n",
    "arg_stance_pipeline = TextClassificationPipeline(model=stance_classifier_teacher_model, tokenizer=stance_classifier_teacher_tokenizer, framework='pt', task='stance_classification', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our model versions with the best performing checkpoint on the validation set..\n",
    "model_without_stance    = OurModel.load('../../multitask-counter-arg-generation/data/output/ca-final-models/mt-v4.baseline_1/trained_models/models-global-step-5000', 'facebook/bart-large',  model_config=transformers.AutoConfig.from_pretrained('facebook/bart-large'))\n",
    "model_with_stance = OurModel.load('../../multitask-counter-arg-generation/data/output/ca-final-models/mt-v4.baseline_2/trained_models/models-global-step-4000', 'facebook/bart-large',  model_config=transformers.AutoConfig.from_pretrained('facebook/bart-large'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "model_without_stance.to(device)\n",
    "model_with_stance.to(device)\n",
    "\n",
    "_ = model_with_stance.eval()\n",
    "_ = model_without_stance.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def get_stance_scores(sents1, sents2):\n",
    "    #compute stance score using our trained model\n",
    "    text_inputs = [x[0] + ' </s> ' + x[1] for x in zip(sents1, sents2)]\n",
    "    stance_results = arg_stance_pipeline(text_inputs, truncation=True)\n",
    "    stance_labels = [int(x['label'].split('_')[-1]) for x in stance_results]\n",
    "    stance_scores = [x['score'] for x in stance_results]\n",
    "    return sum(stance_labels)/len(stance_labels), stance_labels, stance_scores  #The score is the percentage of cases we generated a counter\n",
    "\n",
    "def counters_coherence(post_conclusions, post_counters):\n",
    "    post_counters = [nltk.sent_tokenize(x) for x in post_counters]\n",
    "    conclusion_counter_sent_pairs = [(x[1], s) for x in zip(post_counters, post_conclusions) for s in x[0]]\n",
    "    #print(conclusion_counter_sent_pairs)\n",
    "    conclusions, counter_sents = zip(*conclusion_counter_sent_pairs)\n",
    "    _, stance_labels, stance_scores = get_stance_scores(conclusions, counter_sents)\n",
    "    stance_scores = [x[0] * -1 if x[1] == 0 else x[0] for x in zip(stance_scores, stance_labels)]\n",
    "\n",
    "    #collect counter_scores\n",
    "    counter_scores = []\n",
    "    idx = 0\n",
    "    #print(len(stance_scores))\n",
    "    for i, post_counter in enumerate(post_counters):\n",
    "        #print(len(post_counter))\n",
    "        counter_scores.append(stance_scores[idx: idx + len(post_counter)])\n",
    "        idx+=len(post_counter)\n",
    "    \n",
    "    #print(counter_scores)\n",
    "    return [np.mean(s) for s in counter_scores]\n",
    "\n",
    "def get_best_counters(conclusions, counters, num_sequences):\n",
    "    #choose best counter\n",
    "    best_counters = []\n",
    "    for chunk in chunks(list(zip(conclusions, counters)), num_sequences):\n",
    "        chunk_conclusions, chunk_counters = zip(*chunk)\n",
    "        scores = counters_coherence(chunk_conclusions, chunk_counters)\n",
    "        best_counters.append((chunk_conclusions[np.argmax(scores)], chunk_counters[np.argmax(scores)]))\n",
    "        \n",
    "    return best_counters\n",
    "\n",
    "def generate_counters(model, tokenizer, data_loader, argument_gen_kwargs, conclusion_gen_kwargs, skip_special_tokens=True):\n",
    "    processors = LogitsProcessorList()\n",
    "    generated_counter_arguments = []\n",
    "    generated_conclusions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            generated_argument_tokens   = model.generate_counter_argument(input_ids, attention_mask, argument_gen_kwargs, processors)\n",
    "            generated_conclusion_tokens = model.generate_conclusion(input_ids, attention_mask, conclusion_gen_kwargs, processors)\n",
    "                        \n",
    "            generated_argument_tokens = generated_argument_tokens.cpu().numpy()\n",
    "            decoded_arguments = tokenizer.batch_decode(generated_argument_tokens, skip_special_tokens=skip_special_tokens)\n",
    "            \n",
    "            generated_conclusion_tokens = generated_conclusion_tokens.cpu().numpy()\n",
    "            decoded_conclusions = tokenizer.batch_decode(generated_conclusion_tokens, skip_special_tokens=skip_special_tokens)\n",
    "            \n",
    "            generated_counter_arguments +=decoded_arguments\n",
    "            generated_conclusions +=decoded_conclusions\n",
    "            \n",
    "    return generated_conclusions, generated_counter_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion_gen_kwargs = {\n",
    "    \"do_sample\": False, \n",
    "    \"min_length\":30,\n",
    "    \"top_p\":0.95, \n",
    "    \"num_beams\":10,\n",
    "    \"num_return_sequences\":10\n",
    "}\n",
    "\n",
    "\n",
    "argument_gen_kwargs = {\n",
    "    \"do_sample\": True, \n",
    "    \"max_length\":100,\n",
    "    \"min_length\":50,\n",
    "    \"top_p\":0.95, \n",
    "    \"no_repeat_ngram_size\":3,\n",
    "    \"top_k\": 50,\n",
    "    \"num_beams\":10,\n",
    "    \"num_return_sequences\":10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_dir + 'test_conclusion_all_preprocessed.pkl')[['post_id', 'title', 'post', 'counter']]\n",
    "#df = pd.read_pickle(data_dir + 'sample_test_conclusion_all_preprocessed.pkl')[['post_id', 'title', 'post', 'counter']]\n",
    "df['post'] = df.post.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7fa9bbbd9d30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d009dce63bc4acb99b5447983dd4da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df[['post']])\n",
    "ds = ds.map(lambda a: tokenizer(a['post'], padding='max_length', max_length=256, truncation=True), \n",
    "                                   remove_columns=[ '__index_level_0__'], batched=True)\n",
    "\n",
    "ds.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "dataloader = torch.utils.data.DataLoader(ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_stance_conclusions, no_stance_counter_arguments = generate_counters(model_without_stance, tokenizer, dataloader, argument_gen_kwargs, conclusion_gen_kwargs)\n",
    "#stance_conclusions, stance_counter_arguments = generate_counters(model_with_stance, tokenizer, dataloader, argument_gen_kwargs, conclusion_gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_no_stance_conclusions, best_no_stance_counters = zip(*get_best_counters(no_stance_conclusions, no_stance_counter_arguments, argument_gen_kwargs['num_return_sequences']))\n",
    "#best_stance_conclusions, best_stance_counters = zip(*get_best_counters(stance_conclusions, stance_counter_arguments, argument_gen_kwargs['num_return_sequences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_pred_counter_arguments_no_stance'] = list(chunks(no_stance_counter_arguments, argument_gen_kwargs['num_return_sequences']))\n",
    "df['all_pred_conclusions_no_stance'] = list(chunks(no_stance_conclusions, argument_gen_kwargs['num_return_sequences']))\n",
    "\n",
    "df['pred_counter_arguments_no_stance'] = best_no_stance_counters\n",
    "df['pred_conclusions_no_stance'] = best_no_stance_conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['title', 'pred_conclusions_no_stance', 'pred_counter_arguments_no_stance']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../data/output/ca-final-models/mt-v4/results/all_test_preds_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict single counters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion_gen_kwargs = {\n",
    "    \"do_sample\": False, \n",
    "    #\"max_length\":20,\n",
    "    \"min_length\":30,\n",
    "    \"top_p\":0.95, \n",
    "    \"num_beams\":1,\n",
    "    \"num_return_sequences\":1\n",
    "}\n",
    "\n",
    "argument_gen_kwargs = {\n",
    "    \"do_sample\": True, \n",
    "    \"max_length\":100,\n",
    "    \"min_length\":50,\n",
    "    \"top_p\":0.95, \n",
    "    \"top_k\": 50,\n",
    "    \"no_repeat_ngram_size\":3,\n",
    "    \"num_beams\":4,\n",
    "    \"num_return_sequences\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stance_conclusions, no_stance_counter_arguments = generate_counters(model_without_stance, tokenizer, dataloader, argument_gen_kwargs, conclusion_gen_kwargs)\n",
    "#stance_conclusions, stance_counter_arguments       = generate_counters(model_with_stance, tokenizer, dataloader, argument_gen_kwargs, conclusion_gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['single_pred_counter_arguments_no_stance'] = no_stance_counter_arguments\n",
    "#df['single_pred_counter_arguments_stance'] = stance_counter_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['title', 'single_pred_counter_arguments_no_stance' , 'pred_counter_arguments_no_stance']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../data/output/ca-final-models/mt-v4/results/all_test_preds_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
